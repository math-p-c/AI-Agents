# Tutorial Completo de LangChain e LangGraph v1.0+

**Desenvolvimento de Agentes Inteligentes com Python**

---

## Sum√°rio

### Parte I: Fundamentos
1. [Introdu√ß√£o ao Ecossistema LangChain](#cap√≠tulo-1-introdu√ß√£o-ao-ecossistema-langchain)
2. [Mensagens e Prompts](#cap√≠tulo-2-mensagens-e-prompts)
3. [Tools (Ferramentas)](#cap√≠tulo-3-tools-ferramentas)

### Parte II: LCEL e Composi√ß√£o Declarativa
4. [LangChain Expression Language (LCEL)](#cap√≠tulo-4-langchain-expression-language-lcel)
5. [Modularidade e Interoperabilidade](#cap√≠tulo-5-modularidade-e-interoperabilidade)

### Parte III: Introdu√ß√£o ao LangGraph
6. [Introdu√ß√£o ao LangGraph v1.0+](#cap√≠tulo-6-introdu√ß√£o-ao-langgraph-v10)
7. [Persist√™ncia e Mem√≥ria (Checkpointing)](#cap√≠tulo-7-persist√™ncia-e-mem√≥ria-checkpointing-v10)
8. [Streaming de Respostas](#cap√≠tulo-8-streaming-de-respostas)
9. [Human-in-the-Loop (HITL)](#cap√≠tulo-9-human-in-the-loop-hitl)
10. [Map-Reduce e Paraleliza√ß√£o Din√¢mica](#cap√≠tulo-10-map-reduce-e-paraleliza√ß√£o-din√¢mica)

### Parte IV: Padr√µes Avan√ßados
11. [Middleware e Guardrails em Agentes](#cap√≠tulo-11-middleware-e-guardrails-em-agentes)
12. [Deep Agents - Os 4 Pilares da IA Ag√™ntica Avan√ßada](#cap√≠tulo-12-deep-agents---os-4-pilares-da-ia-ag√™ntica-avan√ßada)
13. [Observabilidade em Agentes com LangSmith](#cap√≠tulo-13-observabilidade-em-agentes-com-langsmith)

### Parte V: RAG - Retrieval Augmented Generation
14. [Fundamentos de RAG](#cap√≠tulo-14-fundamentos-de-rag)
15. [Processamento de PDFs](#cap√≠tulo-15-processamento-de-pdfs)
16. [RAG Ag√™ntico](#cap√≠tulo-16-rag-ag√™ntico)

### Parte VI: Estudo de Caso - TaskMaster CLI
17. [Arquitetura do Projeto](#cap√≠tulo-17-arquitetura-do-projeto-taskmaster-cli)
18. [Implementa√ß√£o do Agente](#cap√≠tulo-18-implementa√ß√£o-do-agente)
19. [Implementa√ß√£o do RAG](#cap√≠tulo-19-implementa√ß√£o-do-rag)
20. [Interface de Terminal com Rich](#cap√≠tulo-20-interface-de-terminal-com-rich)

---

# PARTE I: FUNDAMENTOS

---

## Cap√≠tulo 1: Introdu√ß√£o ao Ecossistema LangChain

### 1.1 O Que S√£o LLMs e Por Que Precisamos de Frameworks?

**Large Language Models (LLMs)** s√£o modelos de intelig√™ncia artificial treinados em vastas quantidades de texto para compreender e gerar linguagem natural. Exemplos incluem GPT-4, Claude, Llama e Gemini.

Embora as APIs desses modelos sejam poderosas, construir aplica√ß√µes robustas diretamente sobre elas apresenta desafios:

- **Gerenciamento de conversas**: Manter hist√≥rico, contexto e estado
- **Integra√ß√£o com ferramentas**: Permitir que o modelo execute a√ß√µes no mundo real
- **Tratamento de erros**: Lidar com falhas, timeouts e respostas inesperadas
- **Orquestra√ß√£o complexa**: Coordenar m√∫ltiplas chamadas e decis√µes

√â aqui que frameworks como **LangChain** e **LangGraph** entram em cena.

### 1.2 LangChain vs LangGraph: Qual a Diferen√ßa?

| Aspecto | LangChain | LangGraph |
|---------|-----------|-----------|
| **Foco** | Componentes e primitivos | Orquestra√ß√£o e fluxo |
| **Abstra√ß√£o** | Mensagens, prompts, tools | Grafos de estado, n√≥s, arestas |
| **Uso** | Blocos de constru√ß√£o | Coordena√ß√£o de agentes |
| **Analogia** | "Pe√ßas de Lego" | "Manual de montagem" |

**LangChain v1.0+** fornece os componentes fundamentais:
- `ChatOpenAI`, `ChatAnthropic` - Interfaces para LLMs
- `SystemMessage`, `HumanMessage`, `AIMessage` - Tipos de mensagens
- `@tool` - Decorator para criar ferramentas
- Schemas Pydantic para valida√ß√£o

**LangGraph v1.0+** fornece a orquestra√ß√£o:
- `StateGraph` - Grafos de estado para fluxos complexos
- Checkpointers - Persist√™ncia e recupera√ß√£o de estado
- Streaming - Respostas em tempo real
- Human-in-the-Loop - Interven√ß√£o humana

**Resumo**: Use LangChain para os **componentes** e LangGraph para **coorden√°-los**.

### 1.3 Configura√ß√£o do Ambiente

#### Pr√©-requisitos
- Python 3.10 ou superior
- Uma chave de API da OpenAI (ou outro provedor)

#### Criando o Projeto

**Com uv (gerenciador de pacotes recomendado)**:
```bash
# Instalar uv (se ainda n√£o tiver)
curl -LsSf https://astral.sh/uv/install.sh | sh
# ou no Windows:
powershell -ExecutionPolicy ByPass -c "irm https://astral.sh/uv/install.ps1 | iex"

# Criar projeto e instalar depend√™ncias
uv init meu_agente
cd meu_agente
uv add langchain langchain-openai langgraph langchain-anthropic langchain-google-genai python-dotenv
```

**Criar o arquivo requirements.txt**:
```bash
uv pip freeze > requirements.txt
```

**Alternativa com pip e venv**:
```bash
# Criar diret√≥rio do projeto
mkdir meu_agente
cd meu_agente

# Criar ambiente virtual
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Instalar depend√™ncias com pip
pip install langchain>=1.0.0 langchain-openai>=0.3.0 langgraph>=1.0.0 python-dotenv
```

#### Configurando Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# .env
# OpenAI
OPENAI_API_KEY=sua-chave-openai-aqui
OPENAI_MODEL=gpt-4o-mini

# Anthropic
ANTHROPIC_API_KEY=sua-chave-anthropic-aqui
ANTHROPIC_MODEL=claude-4-5-haiku

# Google
GOOGLE_API_KEY=sua-chave-google-aqui
GOOGLE_MODEL=gemini-2.5-flush-lite
```

> **Importante**: Nunca commite o arquivo `.env` no Git! Adicione-o ao `.gitignore`.

#### Estrutura de Diret√≥rios do Projeto

Para organizar os exemplos de c√≥digo de cada cap√≠tulo, crie a seguinte estrutura de pastas:

```bash
# Criar pasta src e subpastas para cada cap√≠tulo
mkdir -p src/{ch01,ch02,ch03,ch04,ch05,ch06,ch07,ch08,ch09,ch10,ch11,ch12,ch13,ch14,ch15,ch16,ch17,ch18,ch19,ch20}
```

A estrutura resultante ser√°:
```
meu_agente/
‚îú‚îÄ‚îÄ .env
‚îú‚îÄ‚îÄ requirements.txt
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ pyproject.toml (gerado pelo uv)
‚îú‚îÄ‚îÄ uv.lock
‚îî‚îÄ‚îÄ src/
    ‚îú‚îÄ‚îÄ ch01/
    ‚îú‚îÄ‚îÄ ch02/
    ‚îú‚îÄ‚îÄ ch03/
    ‚îú‚îÄ‚îÄ ch04/
    ‚îú‚îÄ‚îÄ ch05/
    ‚îú‚îÄ‚îÄ ch06/
    ‚îú‚îÄ‚îÄ ch07/
    ‚îú‚îÄ‚îÄ ch08/
    ‚îú‚îÄ‚îÄ ch09/
    ‚îú‚îÄ‚îÄ ch10/
    ‚îú‚îÄ‚îÄ ch11/
    ‚îú‚îÄ‚îÄ ch12/
    ‚îú‚îÄ‚îÄ ch13/
    ‚îú‚îÄ‚îÄ ch14/
    ‚îú‚îÄ‚îÄ ch15/
    ‚îú‚îÄ‚îÄ ch16/
    ‚îú‚îÄ‚îÄ ch17/
    ‚îú‚îÄ‚îÄ ch18/
    ‚îú‚îÄ‚îÄ ch19/
    ‚îî‚îÄ‚îÄ ch20/
```

Cada pasta `chXX` ser√° usada para armazenar os exemplos de c√≥digo referentes ao cap√≠tulo correspondente.

### 1.4 Primeira Chamada a um LLM

Vamos criar nosso primeiro programa que se comunica com um LLM.

```python
# /src/ch01/hello_llm.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Criar inst√¢ncia do modelo
modelo = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    temperature=0  # 0 = determin√≠stico, 1 = criativo
)

# Criar uma mensagem do usu√°rio
mensagem = HumanMessage(content="Ol√°! Qual √© a capital do Brasil?")

# Invocar o modelo
resposta = modelo.invoke([mensagem])

# Exibir a resposta
print(f"Resposta do modelo: {resposta.content}")
```

**Executando**:
```bash
python hello_llm.py
# Sa√≠da: Resposta do modelo: A capital do Brasil √© Bras√≠lia.
```

#### Alternativa: Usando Modelos da Anthropic (Claude)

Se voc√™ preferir usar Claude, instale o pacote `langchain-anthropic`:

```bash
pip install langchain-anthropic
```

Depois, voc√™ pode usar Claude da seguinte forma:

```python
# /src/ch01/hello_llm_anthropic.py
import os
from dotenv import load_dotenv
from langchain_anthropic import ChatAnthropic
from langchain_core.messages import HumanMessage

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Criar inst√¢ncia do modelo Claude
modelo = ChatAnthropic(
    model=os.getenv("ANTHROPIC_MODEL", "claude-4-5-haiku"),
    temperature=0  # 0 = determin√≠stico, 1 = criativo
)

# Criar uma mensagem do usu√°rio
mensagem = HumanMessage(content="Ol√°! Qual √© a capital do Brasil?")

# Invocar o modelo
resposta = modelo.invoke([mensagem])

# Exibir a resposta
print(f"Resposta do modelo: {resposta.content}")
```

#### Alternativa: Usando Modelos do Google (Gemini)

Se voc√™ preferir usar Gemini, instale o pacote `langchain-google-genai`:

```bash
pip install langchain-google-genai
```

Depois, voc√™ pode usar Gemini da seguinte forma:

```python
# /src/ch01/hello_llm_google.py
import os
from dotenv import load_dotenv
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain_core.messages import HumanMessage

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Criar inst√¢ncia do modelo Gemini
modelo = ChatGoogleGenerativeAI(
    model=os.getenv("GOOGLE_MODEL", "gemini-2.0-flash"),
    temperature=0  # 0 = determin√≠stico, 1 = criativo
)

# Criar uma mensagem do usu√°rio
mensagem = HumanMessage(content="Ol√°! Qual √© a capital do Brasil?")

# Invocar o modelo
resposta = modelo.invoke([mensagem])

# Exibir a resposta
print(f"Resposta do modelo: {resposta.content}")
```

> **Dica**: O LangChain oferece uma interface uniforme para diferentes provedores. Uma vez que voc√™ compreender como usar um modelo (OpenAI, Anthropic ou Google), trocar entre eles √© t√£o simples quanto mudar a importa√ß√£o e instanciar uma classe diferente!

### 1.5 Entendendo o C√≥digo

Vamos analisar cada parte:

#### 1. Importa√ß√µes
```python
from langchain_openai import ChatOpenAI          # Interface para OpenAI
from langchain_core.messages import HumanMessage # Tipo de mensagem
```

O pacote `langchain_openai` √© separado do `langchain` principal. Isso segue o padr√£o modular do LangChain v1.0+, onde cada provedor tem seu pr√≥prio pacote.

#### 2. Cria√ß√£o do Modelo
```python
modelo = ChatOpenAI(
    model="gpt-4o-mini",  # Modelo a usar
    temperature=0          # Controle de aleatoriedade
)
```

Par√¢metros importantes:
- `model`: Qual modelo usar (gpt-4o, gpt-4o-mini, gpt-3.5-turbo)
- `temperature`: 0.0 (determin√≠stico) a 1.0 (criativo)
- `max_tokens`: Limite de tokens na resposta
- `timeout`: Tempo m√°ximo de espera

#### 3. Mensagens
```python
mensagem = HumanMessage(content="Ol√°!")
resposta = modelo.invoke([mensagem])
```

O modelo recebe uma **lista de mensagens**. Isso √© fundamental para manter conversas, como veremos no pr√≥ximo cap√≠tulo.

### 1.6 Exemplo Completo: Assistente Simples

Vamos criar um assistente que responde perguntas em um loop:

```python
# /src/ch01/assistente_simples.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

def criar_assistente():
    """Cria e retorna uma inst√¢ncia do modelo."""
    return ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
        temperature=0.7
    )

def conversar(modelo, pergunta: str) -> str:
    """Envia uma pergunta ao modelo e retorna a resposta."""
    mensagens = [
        SystemMessage(content="Voc√™ √© um assistente prestativo que responde em portugu√™s."),
        HumanMessage(content=pergunta)
    ]
    resposta = modelo.invoke(mensagens)
    return resposta.content

def main():
    print("=== Assistente Simples ===")
    print("Digite 'sair' para encerrar.\n")

    modelo = criar_assistente()

    while True:
        pergunta = input("Voc√™: ").strip()

        if pergunta.lower() == 'sair':
            print("At√© logo!")
            break

        if not pergunta:
            continue

        resposta = conversar(modelo, pergunta)
        print(f"Assistente: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Executando**:
```bash
python assistente_simples.py
```

```
=== Assistente Simples ===
Digite 'sair' para encerrar.

Voc√™: Qual √© o maior planeta do sistema solar?
Assistente: O maior planeta do sistema solar √© J√∫piter.

Voc√™: E qual √© o menor?
Assistente: O menor planeta do sistema solar √© Merc√∫rio.

Voc√™: sair
At√© logo!
```

> **Nota**: Este assistente ainda n√£o tem mem√≥ria - cada pergunta √© independente. No pr√≥ximo cap√≠tulo, aprenderemos como manter o contexto da conversa.

### 1.7 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- **LLMs** s√£o modelos de linguagem que compreendem e geram texto
- **LangChain** fornece componentes (mensagens, modelos, tools)
- **LangGraph** fornece orquestra√ß√£o (grafos, estado, persist√™ncia)
- Como **configurar o ambiente** com Python e vari√°veis de ambiente
- Como fazer sua **primeira chamada** a um LLM
- A estrutura b√°sica de **mensagens** (HumanMessage, SystemMessage)

### 1.8 Exerc√≠cios

1. **Modifique o assistente** para usar `temperature=0` e depois `temperature=1`. Observe as diferen√ßas nas respostas.

2. **Crie um tradutor** que receba texto em portugu√™s e traduza para ingl√™s.

3. **Experimente diferentes modelos**: Troque `gpt-4o-mini` por `gpt-4o` e compare a qualidade das respostas.

---

## Cap√≠tulo 2: Mensagens e Prompts

### 2.1 Tipos de Mensagens no LangChain

No LangChain v1.0+, a comunica√ß√£o com LLMs √© baseada em **mensagens tipadas**. Cada tipo de mensagem tem um papel espec√≠fico na conversa:

```python
from langchain_core.messages import (
    SystemMessage,   # Instru√ß√µes do sistema
    HumanMessage,    # Mensagens do usu√°rio
    AIMessage,       # Respostas do assistente
    ToolMessage,     # Resultados de ferramentas
)
```

#### SystemMessage - Definindo o Comportamento

A `SystemMessage` define **quem o assistente √©** e **como ele deve se comportar**. √â sempre a primeira mensagem da conversa.

```python
from langchain_core.messages import SystemMessage

system = SystemMessage(content="""
Voc√™ √© um assistente especializado em programa√ß√£o Python.
Responda sempre em portugu√™s brasileiro.
Seja conciso e forne√ßa exemplos de c√≥digo quando apropriado.
""")
```

#### HumanMessage - Entrada do Usu√°rio

A `HumanMessage` representa as mensagens enviadas pelo usu√°rio:

```python
from langchain_core.messages import HumanMessage

pergunta = HumanMessage(content="Como criar uma lista em Python?")
```

#### AIMessage - Resposta do Assistente

A `AIMessage` representa as respostas geradas pelo modelo. Voc√™ a recebe como retorno do `invoke()`:

```python
resposta = modelo.invoke([system, pergunta])
# resposta √© uma AIMessage
print(type(resposta))  # <class 'langchain_core.messages.ai.AIMessage'>
print(resposta.content)  # "Para criar uma lista em Python..."
```

#### ToolMessage - Resultado de Ferramentas

A `ToolMessage` carrega o resultado da execu√ß√£o de uma ferramenta. Veremos isso em detalhes no Cap√≠tulo 3.

```python
from langchain_core.messages import ToolMessage

resultado = ToolMessage(
    content="A tarefa foi criada com sucesso.",
    tool_call_id="call_abc123"  # ID da chamada da ferramenta
)
```

### 2.2 Estrutura de uma Conversa

Uma conversa √© uma **lista de mensagens** que cresce ao longo do tempo:

```python
# /src/ch02/conversa_estruturada.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

load_dotenv()

modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

# Hist√≥rico da conversa
historico = [
    SystemMessage(content="Voc√™ √© um professor de hist√≥ria. Responda em portugu√™s."),
    HumanMessage(content="Quem descobriu o Brasil?"),
    AIMessage(content="O Brasil foi oficialmente descoberto por Pedro √Ålvares Cabral em 22 de abril de 1500."),
    HumanMessage(content="E em que cidade ele desembarcou?"),
]

# O modelo tem acesso a todo o hist√≥rico
resposta = modelo.invoke(historico)
print(resposta.content)
# Sa√≠da: Pedro √Ålvares Cabral desembarcou na regi√£o que hoje √© Porto Seguro, na Bahia.
```

> **Importante**: O modelo n√£o tem mem√≥ria interna. Voc√™ precisa enviar **todo o hist√≥rico** a cada chamada para manter o contexto.

### 2.3 Chatbot com Mem√≥ria Manual

Vamos criar um chatbot que mant√©m o hist√≥rico da conversa:

```python
# /src/ch02/chatbot_com_memoria.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

class Chatbot:
    def __init__(self, instrucoes: str):
        self.modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7
        )
        self.historico = [SystemMessage(content=instrucoes)]

    def conversar(self, mensagem: str) -> str:
        # Adicionar mensagem do usu√°rio ao hist√≥rico
        self.historico.append(HumanMessage(content=mensagem))

        # Obter resposta do modelo
        resposta = self.modelo.invoke(self.historico)

        # Adicionar resposta ao hist√≥rico
        self.historico.append(resposta)

        return resposta.content

    def limpar_historico(self):
        # Mant√©m apenas a SystemMessage
        self.historico = [self.historico[0]]

def main():
    bot = Chatbot(
        instrucoes="Voc√™ √© um assistente amig√°vel. Responda em portugu√™s de forma concisa."
    )

    print("=== Chatbot com Mem√≥ria ===")
    print("Comandos: 'sair' para encerrar, 'limpar' para reiniciar conversa\n")

    while True:
        entrada = input("Voc√™: ").strip()

        if entrada.lower() == 'sair':
            print("At√© logo!")
            break
        elif entrada.lower() == 'limpar':
            bot.limpar_historico()
            print("Hist√≥rico limpo!\n")
            continue
        elif not entrada:
            continue

        resposta = bot.conversar(entrada)
        print(f"Bot: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Testando a mem√≥ria**:
```
Voc√™: Meu nome √© Jo√£o
Bot: Ol√°, Jo√£o! Prazer em conhec√™-lo. Como posso ajud√°-lo hoje?

Voc√™: Qual √© o meu nome?
Bot: Seu nome √© Jo√£o, conforme voc√™ me disse agora h√° pouco.
```

### 2.4 Templates de Prompt Din√¢micos

Muitas vezes precisamos criar prompts com vari√°veis din√¢micas. O LangChain oferece templates para isso:

```python
# /src/ch02/prompt_dinamico.py
from datetime import datetime
from langchain_core.prompts import ChatPromptTemplate

# Template com vari√°veis
template = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um assistente pessoal.
Data atual: {data_atual}
Nome do usu√°rio: {nome_usuario}
Responda sempre de forma personalizada."""),
    ("human", "{pergunta}")
])

# Preencher as vari√°veis
mensagens = template.invoke({
    "data_atual": datetime.now().strftime("%d/%m/%Y"),
    "nome_usuario": "Maria",
    "pergunta": "Que dia √© hoje?"
})

print(mensagens)
# Sa√≠da: lista de mensagens com as vari√°veis substitu√≠das
```

### 2.5 Inje√ß√£o de Contexto: Data e Hora Atual

Um padr√£o comum √© injetar informa√ß√µes din√¢micas no system prompt. Veja como o projeto JarvisChat faz isso:

```python
# /src/ch02/prompt_com_data.py
import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

def get_system_prompt() -> str:
    """
    Gera o system prompt com data/hora atual.
    IMPORTANTE: Chamar a cada invoca√ß√£o para garantir data atualizada.
    """
    agora = datetime.now()

    # Tradu√ß√£o dos dias da semana
    dias_semana = {
        'Monday': 'segunda-feira',
        'Tuesday': 'ter√ßa-feira',
        'Wednesday': 'quarta-feira',
        'Thursday': 'quinta-feira',
        'Friday': 'sexta-feira',
        'Saturday': 's√°bado',
        'Sunday': 'domingo'
    }

    dia_semana = dias_semana.get(agora.strftime('%A'), agora.strftime('%A'))
    data_formatada = agora.strftime('%d/%m/%Y')
    hora_formatada = agora.strftime('%H:%M')

    return f"""Voc√™ √© um assistente pessoal inteligente.

## Informa√ß√µes Temporais
- Data atual: {data_formatada} ({dia_semana})
- Hora atual: {hora_formatada}

## Instru√ß√µes
- Responda sempre em portugu√™s brasileiro
- Use as informa√ß√µes temporais quando relevante
- Seja cordial e prestativo
"""

def main():
    modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

    mensagens = [
        SystemMessage(content=get_system_prompt()),
        HumanMessage(content="Que dia √© hoje? E que horas s√£o?")
    ]

    resposta = modelo.invoke(mensagens)
    print(resposta.content)

if __name__ == "__main__":
    main()
```

**Sa√≠da exemplo**:
```
Hoje √© 10/12/2025, uma ter√ßa-feira, e s√£o aproximadamente 15:30.
```

### 2.6 Boas Pr√°ticas para System Prompts

Um bom system prompt deve ser:

1. **Claro e espec√≠fico**: Defina exatamente o papel do assistente
2. **Estruturado**: Use se√ß√µes e formata√ß√£o
3. **Contextualizado**: Inclua informa√ß√µes relevantes (data, usu√°rio, etc.)
4. **Com exemplos**: Mostre o formato esperado das respostas

**Exemplo de system prompt bem estruturado**:

```python
SYSTEM_PROMPT = """
# Papel
Voc√™ √© um assistente de gerenciamento de tarefas.

# Capacidades
- Criar, listar, atualizar e excluir tarefas
- Organizar tarefas por categorias
- Definir datas de vencimento

# Regras
1. Sempre confirme a√ß√µes destrutivas (exclus√£o)
2. Use formato de data brasileiro (DD/MM/AAAA)
3. Seja conciso nas respostas

# Formato de Resposta
- Para listagem: use bullets (-)
- Para confirma√ß√µes: use ‚úì ou ‚úó
- Para datas: sempre em portugu√™s

# Contexto
Data atual: {data_atual}
Usu√°rio: {nome_usuario}
"""
```

### 2.7 Exemplo Completo: Assistente com Contexto

```python
# /src/ch02/assistente_contextualizado.py
import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

class AssistenteContextualizado:
    def __init__(self, nome_usuario: str):
        self.modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7
        )
        self.nome_usuario = nome_usuario
        self.historico = []

    def _get_system_prompt(self) -> str:
        agora = datetime.now()
        return f"""Voc√™ √© um assistente pessoal chamado Jarvis.

## Contexto
- Usu√°rio: {self.nome_usuario}
- Data: {agora.strftime('%d/%m/%Y')}
- Hora: {agora.strftime('%H:%M')}

## Personalidade
- Seja cordial e use o nome do usu√°rio ocasionalmente
- Responda em portugu√™s brasileiro
- Seja conciso, mas completo
"""

    def conversar(self, mensagem: str) -> str:
        # System prompt atualizado a cada chamada (data/hora atual)
        system = SystemMessage(content=self._get_system_prompt())

        # Montar mensagens: system + hist√≥rico + nova mensagem
        mensagens = [system] + self.historico + [HumanMessage(content=mensagem)]

        # Obter resposta
        resposta = self.modelo.invoke(mensagens)

        # Atualizar hist√≥rico (sem o system, que √© recriado)
        self.historico.append(HumanMessage(content=mensagem))
        self.historico.append(resposta)

        return resposta.content

def main():
    nome = input("Qual √© o seu nome? ").strip() or "Usu√°rio"
    assistente = AssistenteContextualizado(nome_usuario=nome)

    print(f"\nOl√°, {nome}! Sou o Jarvis, seu assistente pessoal.")
    print("Digite 'sair' para encerrar.\n")

    while True:
        entrada = input("Voc√™: ").strip()
        if entrada.lower() == 'sair':
            print(f"At√© logo, {nome}!")
            break
        if entrada:
            resposta = assistente.conversar(entrada)
            print(f"Jarvis: {resposta}\n")

if __name__ == "__main__":
    main()
```

### 2.8 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- Os **4 tipos de mensagens**: SystemMessage, HumanMessage, AIMessage, ToolMessage
- Como **estruturar conversas** com listas de mensagens
- A implementar **mem√≥ria manual** mantendo o hist√≥rico
- A criar **templates din√¢micos** com vari√°veis
- Como **injetar contexto** (data, hora, usu√°rio) no prompt
- **Boas pr√°ticas** para system prompts eficazes

### 2.9 Exerc√≠cios

1. **Crie um chatbot tem√°tico**: Um assistente especializado em receitas culin√°rias que pergunta sobre ingredientes dispon√≠veis.

2. **Limite de contexto**: Modifique o chatbot para manter apenas as √∫ltimas 10 mensagens no hist√≥rico (evitando estourar o limite de tokens).

3. **Prompt multil√≠ngue**: Crie um assistente que detecta o idioma da pergunta e responde no mesmo idioma.

---

## Padr√µes Reutiliz√°veis de C√≥digo LangGraph

Nesta se√ß√£o, voc√™ aprender√° os **padr√µes fundamentais** de c√≥digo LangGraph que ser√£o usados repetidamente nos cap√≠tulos posteriores. Em vez de duplicar essas implementa√ß√µes, os cap√≠tulos posteriores far√£o **refer√™ncia** a esses padr√µes can√¥nicos.

### Padr√£o 1: N√≥ LLM (llm_call)

Esta fun√ß√£o implementa um n√≥ que:
1. Obt√©m as mensagens do estado
2. Garante que existe um SystemMessage (prompt do sistema)
3. Chama o modelo LLM com as ferramentas dispon√≠veis
4. Retorna a resposta como AIMessage

```python
def llm_call(state: AgentState) -> dict:
    """N√≥ que chama o LLM.

    Par√¢metros:
        state (AgentState): Estado atual com lista de mensagens

    Retorna:
        dict: Dicion√°rio com chave 'messages' contendo a resposta do modelo
    """
    messages = state["messages"]

    # Adicionar system prompt se n√£o existir
    if not messages or not isinstance(messages[0], SystemMessage):
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages

    # Chamar modelo
    response = modelo_com_tools.invoke(messages)

    return {"messages": [response]}
```

**Importante:**
- `SYSTEM_PROMPT` deve ser definido como string com as instru√ß√µes do agente
- `modelo_com_tools` deve ser uma inst√¢ncia de modelo LangChain com `.bind_tools()` j√° aplicado
- A vari√°vel `AgentState` deve ser um TypedDict com campo `messages: list`

---

### Padr√£o 2: N√≥ de Ferramentas (tool_node)

Esta fun√ß√£o implementa um n√≥ que:
1. Obt√©m a √∫ltima mensagem (AIMessage com tool_calls)
2. Itera sobre cada chamada de ferramenta
3. Executa a ferramenta usando um mapa `TOOLS_BY_NAME`
4. Retorna ToolMessages com os resultados

```python
def tool_node(state: AgentState) -> dict:
    """N√≥ que executa as tools chamadas pelo LLM.

    Par√¢metros:
        state (AgentState): Estado atual com hist√≥rico de mensagens

    Retorna:
        dict: Dicion√°rio com chave 'messages' contendo ToolMessages
    """
    messages = state["messages"]
    last_message = messages[-1]

    # Processar cada tool_call
    tool_messages = []
    for tool_call in last_message.tool_calls:
        tool_name = tool_call["name"]
        tool_args = tool_call["args"]
        tool_call_id = tool_call["id"]

        # Executar a tool
        try:
            tool_fn = TOOLS_BY_NAME[tool_name]
            result = tool_fn.invoke(tool_args)
        except Exception as e:
            result = f"Erro ao executar {tool_name}: {e}"

        # Criar ToolMessage com o resultado
        tool_messages.append(ToolMessage(
            content=str(result),
            tool_call_id=tool_call_id
        ))

    return {"messages": tool_messages}
```

**Importante:**
- `TOOLS_BY_NAME` deve ser um dicion√°rio: `{"nome_tool": Tool, ...}`
- As Tools devem ser inst√¢ncias de `langchain.tools.Tool` ou decoradas com `@tool`
- O mapa `tool_call_id` conecta a resposta √† solicita√ß√£o original

---

### Padr√£o 3: Fun√ß√£o de Decis√£o (should_continue)

Esta fun√ß√£o implementa uma **fun√ß√£o condicional** que:
1. Obt√©m a √∫ltima mensagem do estado
2. Verifica se cont√©m chamadas de ferramentas (tool_calls)
3. Roteia para `tool_node` se houver tools, ou finaliza (`__end__`) se n√£o houver

```python
def should_continue(state: AgentState) -> Literal["tool_node", "__end__"]:
    """Fun√ß√£o condicional que decide se deve executar tools ou finalizar.

    Par√¢metros:
        state (AgentState): Estado atual com hist√≥rico de mensagens

    Retorna:
        Literal["tool_node", "__end__"]: Nome do pr√≥ximo n√≥ ou sinal de fim
    """
    messages = state["messages"]
    last_message = messages[-1]

    # Se a √∫ltima mensagem tem tool_calls, executar tools
    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tool_node"

    # Caso contr√°rio, finalizar
    return "__end__"
```

**Importante:**
- Esta fun√ß√£o retorna uma string que corresponde ao nome de um n√≥ no grafo
- `"__end__"` √© a constante especial do LangGraph para indicar fim da execu√ß√£o
- Use `Literal` do m√≥dulo `typing` para type hints

---

### Como Usar Estes Padr√µes

Nos cap√≠tulos subsequentes, quando voc√™ vir uma se√ß√£o que implementa um agente LangGraph, ela frequentemente usar√° estes padr√µes. Para us√°-los:

1. **Defina seu AgentState** com os campos necess√°rios (miniscamente, `messages`)
2. **Adapte SYSTEM_PROMPT e TOOLS_BY_NAME** para seu caso de uso espec√≠fico
3. **Copie as tr√™s fun√ß√µes** (llm_call, tool_node, should_continue)
4. **Construa seu grafo** usando o padr√£o de adicionar n√≥s e arestas

Exemplo m√≠nimo:
```python
from langgraph.graph import StateGraph, START, END

# 1. Definir estado
class MyAgentState(TypedDict):
    messages: list

# 2. Configurar modelo e ferramentas
SYSTEM_PROMPT = "Voc√™ √© um assistente √∫til..."
modelo_com_tools = model.bind_tools(tools_list)
TOOLS_BY_NAME = {t.name: t for t in tools_list}

# 3. Copiar as tr√™s fun√ß√µes de padr√£o (llm_call, tool_node, should_continue)

# 4. Construir grafo
graph = StateGraph(MyAgentState)
graph.add_node("llm_call", llm_call)
graph.add_node("tool_node", tool_node)
graph.add_edge(START, "llm_call")
graph.add_conditional_edges("llm_call", should_continue, {
    "tool_node": "tool_node",
    "__end__": END
})
graph.add_edge("tool_node", "llm_call")
app = graph.compile()
```

---

## Cap√≠tulo 3: Tools (Ferramentas)

### 3.1 O Que S√£o Tools e Por Que S√£o Importantes?

**Tools** (ferramentas) s√£o fun√ß√µes que o modelo pode **decidir chamar** para realizar a√ß√µes no mundo real. Sem tools, o modelo √© apenas um gerador de texto. Com tools, ele se torna um **agente** capaz de:

- Buscar informa√ß√µes em bancos de dados
- Fazer c√°lculos matem√°ticos
- Criar, atualizar e excluir dados
- Interagir com APIs externas
- Executar c√≥digo

#### Fluxo de Execu√ß√£o com Tools

```mermaid
flowchart TD
    User["<b>Usu√°rio</b><br/>Calcule 2 + 2"] --> LLM1["<b>LLM</b><br/>(decide usar tool)"]
    LLM1 --> Tool["<b>Tool</b><br/>(executa c√°lculo)"]
    Tool --> Result["<b>Resultado</b><br/>4"]
    Result --> LLM2["<b>LLM</b><br/>(formata resposta)"]
    LLM2 --> Output["<b>Resposta Final</b><br/>2 + 2 = 4"]
```

### 3.2 Criando Tools com o Decorator @tool

O LangChain v1.0+ usa o decorator `@tool` para criar ferramentas:

```python
# /src/ch03/tool_simples.py
from langchain.tools import tool

@tool
def somar(a: int, b: int) -> int:
    """Soma dois n√∫meros inteiros.
    Args:
        a: Primeiro n√∫mero
        b: Segundo n√∫mero
    Returns:
        A soma dos dois n√∫meros
    """
    return a + b


if __name__ == "__main__":
    # Inspecionar a tool
    print(f"Nome: {somar.name}")
    print(f"Descri√ß√£o: {somar.description}")
    print(f"Schema: {somar.args}")

    # Exemplo de uso direto
    resultado = somar.invoke({"a": 5, "b": 3})
    print(f"Resultado: {resultado}")
```

**Sa√≠da**:
```
Nome: somar
Descri√ß√£o: Soma dois n√∫meros inteiros.
    Args:
        a: Primeiro n√∫mero
        b: Segundo n√∫mero
    Returns:
        A soma dos dois n√∫meros
Schema: {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}
Resultado: 8
```

> **Importante**: A **docstring** √© fundamental! O LLM usa a descri√ß√£o para decidir quando usar a tool.

### 3.3 Tools com Schemas Pydantic

Para tools mais complexas, use modelos Pydantic para valida√ß√£o:

```python
# /src/ch03/tool_com_pydantic.py
from langchain.tools import tool
from pydantic import BaseModel, Field
from typing import Optional
from datetime import date

class CriarTarefaInput(BaseModel):
    """Schema para cria√ß√£o de tarefas."""

    titulo: str = Field(description="T√≠tulo da tarefa")
    descricao: Optional[str] = Field(default=None, description="Descri√ß√£o detalhada")
    data_vencimento: Optional[date] = Field(
        default=None, description="Data de vencimento (YYYY-MM-DD)"
    )

@tool(args_schema=CriarTarefaInput)
def criar_tarefa(
    titulo: str,
    descricao: Optional[str] = None,
    data_vencimento: Optional[date] = None,
) -> str:
    """Cria uma nova tarefa no sistema.
    Use esta ferramenta quando o usu√°rio quiser adicionar uma nova tarefa,
    atividade ou lembrete.
    """
    # Simula√ß√£o - em produ√ß√£o, salvaria no banco de dados
    tarefa_id = 123
    resultado = f"Tarefa criada com sucesso!\n"
    resultado += f"- ID: {tarefa_id}\n"
    resultado += f"- T√≠tulo: {titulo}\n"
    if descricao:
        resultado += f"- Descri√ß√£o: {descricao}\n"
    if data_vencimento:
        resultado += f"- Vencimento: {data_vencimento.strftime('%d/%m/%Y')}\n"
    return resultado

if __name__ == "__main__":
    # Testar a tool diretamente
    print(
        criar_tarefa.invoke(
            {
                "titulo": "Estudar LangChain",
                "descricao": "Completar tutorial",
                "data_vencimento": "2025-12-15",
            }
        )
    )

    # Exemplo adicional: inspe√ß√£o da tool
    print(f"\n--- Informa√ß√µes da Tool ---")
    print(f"Nome: {criar_tarefa.name}")
    print(f"Descri√ß√£o: {criar_tarefa.description}")
    print(f"Schema: {criar_tarefa.args}")
```

### 3.4 Binding Tools ao Modelo

Para que o modelo possa usar as tools, precisamos "bind√°-las":

```python
# /src/ch03/binding_tools.py
import os
from dotenv import load_dotenv
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

load_dotenv()

# Definir tools
@tool
def calcular(expressao: str) -> str:
    """Calcula uma express√£o matem√°tica simples.

    Args:
        expressao: Express√£o matem√°tica (ex: "2 + 2", "10 * 5")
    """
    try:
        # ATEN√á√ÉO: eval() √© perigoso em produ√ß√£o!
        # Use uma biblioteca segura como numexpr
        resultado = eval(expressao)
        return f"Resultado: {resultado}"
    except Exception as e:
        return f"Erro no c√°lculo: {e}"

@tool
def obter_clima(cidade: str) -> str:
    """Obt√©m informa√ß√µes sobre o clima de uma cidade.

    Args:
        cidade: Nome da cidade
    """
    # Simula√ß√£o - em produ√ß√£o, chamaria uma API real
    climas = {
        "s√£o paulo": "Nublado, 22¬∞C",
        "rio de janeiro": "Ensolarado, 32¬∞C",
        "curitiba": "Chuvoso, 15¬∞C",
    }
    return climas.get(cidade.lower(), f"Clima n√£o dispon√≠vel para {cidade}")

if __name__ == "__main__":
    # Criar modelo COM tools bindadas
    from langchain_openai import ChatOpenAI
    modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
    modelo_com_tools = modelo.bind_tools([calcular, obter_clima])

    # Testar - o modelo decide qual tool usar
    resposta = modelo_com_tools.invoke([
        HumanMessage(content="Quanto √© 15 vezes 8?")
    ])

    print(f"Conte√∫do: {resposta.content}")
    print(f"Tool calls: {resposta.tool_calls}")
```

**Sa√≠da**:
```
Conte√∫do:
Tool calls: [{'name': 'calcular', 'args': {'expressao': '15 * 8'}, 'id': 'call_abc123', 'type': 'tool_call'}]
```

> **Observe**: Quando o modelo decide usar uma tool, o `content` fica vazio e os argumentos v√£o em `tool_calls`.

### 3.5 Executando Tools e Retornando Resultados

O fluxo completo envolve:
1. Enviar mensagem ao modelo
2. Verificar se h√° tool_calls
3. Executar as tools
4. Enviar resultados de volta ao modelo
5. Obter resposta final

```python
# /src/ch03/executar_tools.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, ToolMessage, AIMessage, BaseMessage

load_dotenv()

@tool
def calcular(expressao: str) -> str:
    """Calcula uma express√£o matem√°tica."""
    try:
        resultado = eval(expressao)
        return str(resultado)
    except Exception as e:
        return f"Erro: {e}"

@tool
def obter_clima(cidade: str) -> str:
    """Obt√©m o clima de uma cidade."""
    climas = {"s√£o paulo": "22¬∞C, Nublado", "rio de janeiro": "32¬∞C, Sol"}
    return climas.get(cidade.lower(), "Dados n√£o dispon√≠veis")

# Mapear tools por nome
tools = [calcular, obter_clima]
tools_por_nome = {t.name: t for t in tools}

# Modelo com tools
modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
modelo_com_tools = modelo.bind_tools(tools)

def processar_com_tools(mensagem: str) -> str:
    """Processa uma mensagem, executando tools se necess√°rio."""
    mensagens: list[BaseMessage] = [HumanMessage(content=mensagem)]

    # Primeira chamada ao modelo
    resposta = modelo_com_tools.invoke(mensagens)
    mensagens.append(resposta)

    # Se houver tool_calls, executar
    while resposta.tool_calls:
        for tool_call in resposta.tool_calls:
            nome_tool = tool_call["name"]
            args = tool_call["args"]
            tool_call_id = tool_call["id"]
            print(f"Executando tool: {nome_tool}({args})")
            # Executar a tool
            tool_fn = tools_por_nome[nome_tool]
            resultado = tool_fn.invoke(args)
            # Adicionar resultado como ToolMessage
            mensagens.append(ToolMessage(content=resultado, tool_call_id=tool_call_id))

        # Nova chamada ao modelo com os resultados
        resposta = modelo_com_tools.invoke(mensagens)
        mensagens.append(resposta)
    return resposta.text

# Testar
print(processar_com_tools("Quanto √© 25 ao quadrado?"))
print("\n---\n")
print(processar_com_tools("Como est√° o clima em S√£o Paulo?"))
```

**Sa√≠da**:
```
Executando tool: calcular({'expressao': '25ÀÜ2'})
25 ao quadrado √© igual a 625.

---

Executando tool: obter_clima({'cidade': 'S√£o Paulo'})
O clima em S√£o Paulo est√° em torno de 22¬∞C, com c√©u nublado.
```

### 3.6 Tool com M√∫ltiplos Par√¢metros

Vamos criar uma tool mais complexa para gerenciar tarefas:

```python
# /src/ch03/tool_tarefas.py
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from typing import Optional, Literal
from datetime import date
from enum import Enum

class EstadoTarefa(str, Enum):
    PENDENTE = "pendente"
    CONCLUIDA = "concluida"
    ARQUIVADA = "arquivada"

class ListarTarefasInput(BaseModel):
    """Schema para listagem de tarefas."""
    estado: Optional[Literal["pendente", "concluida", "arquivada"]] = Field(
        default=None,
        description="Filtrar por estado da tarefa"
    )
    categoria: Optional[str] = Field(
        default=None,
        description="Filtrar por categoria"
    )

# Banco de dados simulado
TAREFAS_DB = [
    {"id": 1, "titulo": "Estudar Python", "estado": "pendente", "categoria": "Estudos"},
    {"id": 2, "titulo": "Fazer compras", "estado": "concluida", "categoria": "Pessoal"},
    {"id": 3, "titulo": "Reuni√£o de equipe", "estado": "pendente", "categoria": "Trabalho"},
]

@tool(args_schema=ListarTarefasInput)
def listar_tarefas(
    estado: Optional[str] = None,
    categoria: Optional[str] = None
) -> str:
    """Lista as tarefas do usu√°rio com filtros opcionais.

    Use esta ferramenta para mostrar tarefas existentes.
    Pode filtrar por estado (pendente, concluida, arquivada) e/ou categoria.
    """
    tarefas = TAREFAS_DB.copy()

    # Aplicar filtros
    if estado:
        tarefas = [t for t in tarefas if t["estado"] == estado]
    if categoria:
        tarefas = [t for t in tarefas if t["categoria"].lower() == categoria.lower()]

    if not tarefas:
        return "Nenhuma tarefa encontrada com os filtros especificados."

    # Formatar resultado
    resultado = f"Encontradas {len(tarefas)} tarefa(s):\n\n"
    for t in tarefas:
        emoji = "‚è≥" if t["estado"] == "pendente" else "‚úÖ" if t["estado"] == "concluida" else "üì¶"
        resultado += f"{emoji} [{t['id']}] {t['titulo']}\n"
        resultado += f"   Categoria: {t['categoria']} | Estado: {t['estado']}\n\n"

    return resultado

# Testar
print(listar_tarefas.invoke({"estado": "pendente"}))
```

### 3.7 Boas Pr√°ticas para Tools

1. **Docstrings descritivas**: O LLM usa a descri√ß√£o para decidir quando usar a tool
2. **Nomes claros**: Use verbos no infinitivo (criar, listar, atualizar, excluir)
3. **Valida√ß√£o com Pydantic**: Garante que os argumentos est√£o corretos
4. **Tratamento de erros**: Retorne mensagens de erro √∫teis, n√£o exce√ß√µes
5. **Retorno informativo**: Confirme o que foi feito, n√£o apenas "sucesso"

```python
# Exemplo de tool bem documentada
@tool
def criar_tarefa(titulo: str, data_vencimento: Optional[str] = None) -> str:
    """Cria uma nova tarefa no sistema de gerenciamento.

    Use esta ferramenta quando o usu√°rio quiser:
    - Adicionar uma nova tarefa
    - Criar um lembrete
    - Agendar uma atividade

    Args:
        titulo: T√≠tulo descritivo da tarefa (obrigat√≥rio)
        data_vencimento: Data limite no formato YYYY-MM-DD (opcional)

    Returns:
        Confirma√ß√£o com detalhes da tarefa criada

    Exemplos de uso:
        - "Crie uma tarefa para estudar Python"
        - "Adicione lembrete: reuni√£o dia 15/12"
    """
    # Implementa√ß√£o...
```

### 3.8 Exemplo Completo: Assistente com Tools

```python
# /src/ch03/assistente_com_tools.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage

load_dotenv()

# === TOOLS ===

@tool
def somar(a: float, b: float) -> str:
    """Soma dois n√∫meros."""
    return str(a + b)

@tool
def subtrair(a: float, b: float) -> str:
    """Subtrai b de a."""
    return str(a - b)

@tool
def multiplicar(a: float, b: float) -> str:
    """Multiplica dois n√∫meros."""
    return str(a * b)

@tool
def dividir(a: float, b: float) -> str:
    """Divide a por b."""
    if b == 0:
        return "Erro: divis√£o por zero n√£o √© permitida"
    return str(a / b)

# === ASSISTENTE ===

class AssistenteCalculadora:
    def __init__(self):
        self.tools = [somar, subtrair, multiplicar, dividir]
        self.tools_por_nome = {t.name: t for t in self.tools}

        modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        self.modelo = modelo.bind_tools(self.tools)

        self.system = SystemMessage(content="""
Voc√™ √© uma calculadora inteligente.
Use as ferramentas dispon√≠veis para fazer c√°lculos.
Sempre mostre o resultado de forma clara.
""")

    def processar(self, pergunta: str) -> str:
        mensagens = [self.system, HumanMessage(content=pergunta)]

        while True:
            resposta = self.modelo.invoke(mensagens)
            mensagens.append(resposta)

            # Se n√£o h√° tool_calls, retornar resposta final
            if not resposta.tool_calls:
                return resposta.content

            # Executar cada tool
            for tool_call in resposta.tool_calls:
                nome = tool_call["name"]
                args = tool_call["args"]

                resultado = self.tools_por_nome[nome].invoke(args)

                mensagens.append(ToolMessage(
                    content=resultado,
                    tool_call_id=tool_call["id"]
                ))

def main():
    calc = AssistenteCalculadora()

    print("=== Calculadora Inteligente ===")
    print("Digite 'sair' para encerrar.\n")

    while True:
        entrada = input("Voc√™: ").strip()
        if entrada.lower() == 'sair':
            break
        if entrada:
            resposta = calc.processar(entrada)
            print(f"Calculadora: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Testando**:
```
Voc√™: Quanto √© 15 vezes 8?
Calculadora: 15 vezes 8 √© igual a 120.

Voc√™: Agora divida o resultado por 3
Calculadora: 120 dividido por 3 √© igual a 40.

Voc√™: Some 100 e 200, depois multiplique por 2
Calculadora: (100 + 200) √ó 2 = 600
```

### 3.9 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- O que s√£o **tools** e por que s√£o essenciais para agentes
- Como criar tools com o **decorator @tool**
- Como usar **Pydantic** para valida√ß√£o de argumentos
- Como fazer **bind_tools()** ao modelo
- O fluxo completo de **execu√ß√£o de tools**
- **Boas pr√°ticas** para documenta√ß√£o de tools

### 3.10 Exerc√≠cios

1. **Crie uma tool de convers√£o**: Converta temperaturas entre Celsius e Fahrenheit.

2. **Tool de busca**: Crie uma tool que simula busca em uma lista de produtos.

3. **M√∫ltiplas tools**: Crie um assistente com tools para CRUD completo de uma entidade.

---

# PARTE II: LCEL E COMPOSI√á√ÉO DECLARATIVA

---

## Cap√≠tulo 4: LangChain Expression Language (LCEL)

### 4.1 O Que √â LCEL?

At√© agora, constru√≠mos pipelines "manualmente": criamos prompts, passamos para modelos, parseamos outputs. √Ä medida que sistemas crescem, esse padr√£o se torna repetitivo e fr√°gil.

**LangChain Expression Language (LCEL)** √© a resposta: uma linguagem declarativa para compor componentes LangChain usando o operador **pipe** (`|`).

> **LCEL** permite construir pipelines reutiliz√°veis, test√°veis e serializ√°veis sem "glue code" manual.

### 4.2 Composi√ß√£o com Pipes

A ideia √© simples: componentes podem ser encadeados com `|`, criando uma pipeline declarativa:

```python
# Antes: manual
prompt = ChatPromptTemplate.from_messages([...])
model = ChatOpenAI(...)
parser = StrOutputParser()

messages = prompt.invoke({"topic": "Python"})
response = model.invoke(messages)
resposta_final = parser.invoke(response)

# Depois: LCEL com pipes
chain = prompt | model | parser
resposta_final = chain.invoke({"topic": "Python"})
```

O operador `|` √© sintaticamente simples, mas semanticamente poderoso: transforma fun√ß√µes em objetos compostos que sabem como:
- Invocar (`.invoke()`)
- Fazer streaming (`.stream()`)
- Executar em batch (`.batch()`)

### 4.3 Runnables em LCEL - Blocos de Constru√ß√£o

LCEL usa o conceito de **Runnable** como bloco fundamental. Entender Runnables √© essencial para compor pipelines.

#### 4.3.1 O que √© um Runnable?

**Defini√ß√£o**:
- Interface padr√£o do LangChain para qualquer componente que pode ser executado
- Permite composi√ß√£o via operador `|`
- Suporta opera√ß√µes padr√£o: `invoke()`, `stream()`, `batch()`

**Exemplos de objetos que s√£o Runnables**:
- Prompts (`ChatPromptTemplate`)
- LLMs (`ChatOpenAI`, `Anthropic`)
- Parsers (`StrOutputParser`)
- Fun√ß√µes Python customizadas (via `RunnableLambda`)
- Dicion√°rios (automaticamente convertidos em `RunnableParallel`)

**Por que Runnables?**
- Interface consistente para todos os componentes
- Permite encadear qualquer coisa com `|`
- Automaticamente suporta async, streaming, batch

#### 4.3.2 RunnablePassthrough

**O que faz?**
- Passa dados **inalterados** atrav√©s da cadeia
- √ötil para preservar inputs originais em pipelines complexos
- N√£o faz nenhuma transforma√ß√£o

**Caso de Uso Principal - Tradu√ß√£o com Preserva√ß√£o**:
```python
# Preserva o texto original enquanto o traduz
pipeline = (
    {"original": RunnablePassthrough(), "traducao": prompt | model | parser}
)
```

**Explica√ß√£o do Fluxo**:
- Dict syntax `{...}` cria um `RunnableParallel`: execu√ß√£o PARALELA
- `RunnablePassthrough()` mant√©m o input original intacto (sem transforma√ß√£o)
- `prompt | model | parser` transforma o input (traduz)
- Ambos os caminhos recebem o mesmo input e executam simultaneamente
- Resultados s√£o mesclados em um dicion√°rio

**Diagrama Visual**:
```mermaid
flowchart TD
    Input["<b>Input</b><br/>texto: Ol√°! Como voc√™ est√°?"] --> Fork{" "}
    Fork --> Path1["<b>RunnablePassthrough()</b><br/>sem modifica√ß√£o"]
    Fork --> Path2["<b>prompt | model | parser</b><br/>traduz"]
    Path1 --> Result1["<b>Original</b><br/>texto: Ol√°! Como voc√™ est√°?"]
    Path2 --> Result2["<b>Tradu√ß√£o</b><br/>Hello! How are you?"]
    Result1 --> Merge["<b>Merge dos resultados</b>"]
    Result2 --> Merge
    Merge --> Output["<b>Output</b><br/>original: {...}<br/>traducao: Hello! How are you?"]
```

**Exemplo Completo - RunnablePassthrough Simples**:

```python
# /src/ch04/runnable_passthrough_simples.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

load_dotenv()

# Template simples para traduzir portugu√™s ‚Üí ingl√™s
prompt_traduzir = ChatPromptTemplate.from_template(
    """Traduza para ingl√™s, mantendo o significado:

{texto}

Tradu√ß√£o:"""
)

model = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), temperature=0)

# Parser: Extrai apenas o texto da resposta
parser = StrOutputParser()

pipeline = RunnableParallel(
    {
        "original": RunnablePassthrough(),  # Preserva o input original
        "traducao": prompt_traduzir | model | parser,  # Traduz o input
    }
)

if __name__ == "__main__":
    # Exemplo de entrada
    texto = {"texto": "Ol√°! Como voc√™ est√°? O clima est√° lindo hoje!"}

    print("Input (original):")
    print(f"  {texto['texto']}")
    print()

    # Executar a pipeline
    resultado = pipeline.invoke(texto)

    print("Output (ap√≥s pipeline):")
    print(f"  Original (preservado): {resultado['original']}")
    print(f"  Tradu√ß√£o (processada): {resultado['traducao']}")
```

**Explica√ß√£o Detalhada do Fluxo**:

1. **Inicializa√ß√£o**:
   - Importamos `RunnableParallel` e `RunnablePassthrough` do `langchain_core.runnables`
   - Configuramos `ChatPromptTemplate` para tradu√ß√£o portugu√™s ‚Üí ingl√™s
   - Instanciamos `ChatOpenAI` com modelo `gpt-4o-mini`
   - Criamos `StrOutputParser` para extrair apenas texto da resposta

2. **Cria√ß√£o da Pipeline**:
   ```python
   pipeline = RunnableParallel(
       {
           "original": RunnablePassthrough(),  # Preserva o input original
           "traducao": prompt_traduzir | model | parser,  # Traduz o input
       }
   )
   ```
   - O **dict syntax** `{...}` √© passado para um `RunnableParallel`
   - Dois caminhos executam **simultaneamente** (execu√ß√£o paralela)
   - Ambos recebem o **mesmo input** `{"texto": "..."}`

3. **Execu√ß√£o**:
   ```python
   resultado = pipeline.invoke(texto)
   ```
   - A pipeline executa ambos os caminhos em paralelo
   - Retorna um dicion√°rio mesclado com ambos os resultados

4. **Resultado Final**:
   ```
   {
       "original": {"texto": "Ol√°! Como voc√™ est√°? ..."},
       "traducao": "Hello! How are you? ..."
   }
   ```

**Por que usar RunnablePassthrough?**
- **Preserva√ß√£o de contexto**: Mant√©m dados originais para refer√™ncia
- **Processamento paralelo**: M√∫ltiplas transforma√ß√µes no mesmo input simultaneamente
- **Flexibilidade**: Voc√™ pode usar o original + resultado em etapas subsequentes
- **Sem boilerplate**: N√£o precisa duplicar o input manualmente no c√≥digo

#### 4.3.3 RunnableLambda

**O que faz?**
- Transforma **qualquer fun√ß√£o Python** em um Runnable
- Permite integrar l√≥gica customizada em cadeias LCEL
- A fun√ß√£o fica "native" √† cadeia (suporta stream, batch, etc.)

**Exemplo de Uso**:
```python
def adicionar_timestamp(input):
    from datetime import datetime
    return f"[{datetime.now()}] {input}"

chain = (
    RunnableLambda(adicionar_timestamp)
    | prompt
    | model
)
```

**Quando usar RunnableLambda?**
- Transformar dados entre componentes
- Adicionar logs ou monitoramento
- Aplicar regras de neg√≥cio customizadas
- Integrar com APIs ou bancos de dados externos
- Fazer pr√©/p√≥s-processamento de texto

**Compara√ß√£o**:

| Sem RunnableLambda | Com RunnableLambda |
|-------------------|-------------------|
| Fun√ß√£o separada, fora da cadeia | Integrado √† cadeia |
| Precisa chamar manualmente | Executa automaticamente no fluxo |
| N√£o suporta `.stream()` autom√°tico | Suporta todas opera√ß√µes Runnable |
| C√≥digo mais verboso | C√≥digo mais limpo |

#### 4.3.4 Resumo - Padr√µes Comuns de Runnables

| Runnable | Entrada | Sa√≠da | Caso de Uso Principal |
|----------|---------|-------|----------------------|
| `RunnablePassthrough` | Qualquer input | Input inalterado | Passar dados atrav√©s da cadeia |
| `RunnableLambda` | Qualquer input | Resultado da fun√ß√£o | Transforma√ß√µes customizadas |
| `RunnableParallel` | Dicion√°rio `{...}` | Dict com resultados | Executar m√∫ltiplas cadeias em paralelo |

### 4.4 Exemplo Pr√°tico: Pipeline de Tradu√ß√£o Multi-etapa

Vamos construir uma pipeline de tradu√ß√£o com 4 est√°gios que demonstra todos os conceitos LCEL aprendidos:

A pipeline demonstra:
1. **Composi√ß√£o com pipes** (`|`): Encadear 4 est√°gios sequencialmente
2. **RunnablePassthrough**: Preservar dados originais atrav√©s de est√°gios
3. **RunnableLambda**: Adicionar l√≥gica Python customizada
4. **RunnableParallel (dict)**: Execu√ß√£o paralela de m√∫ltiplos caminhos
5. **Streaming**: Suporte autom√°tico a tokens incrementais

**Os 4 Est√°gios**:

```mermaid
flowchart TD
    subgraph Stage1["<b>Est√°gio 1: Tradu√ß√£o + Preserva√ß√£o</b>"]
        Input1["Input: Texto portugu√™s"] --> Fork1{" "}
        Fork1 --> Pass1["RunnablePassthrough<br/>(preserva original)"]
        Fork1 --> Trans1["prompt | model | parser<br/>(traduz para ingl√™s)"]
        Pass1 --> Out1["Output: {original, traducao}"]
        Trans1 --> Out1
    end

    subgraph Stage2["<b>Est√°gio 2: An√°lise Lingu√≠stica</b>"]
        Input2["Input: {original, traducao}"] --> Analysis["Analisar caracter√≠sticas:<br/>‚Ä¢ formalidade<br/>‚Ä¢ dom√≠nio<br/>‚Ä¢ tom"]
        Analysis --> Out2["Output: {original, traducao, analise}"]
    end

    subgraph Stage3["<b>Est√°gio 3: Ajuste de Tom</b>"]
        Input3["Input: {original, traducao, analise}"] --> Fork3{" "}
        Fork3 --> Pass3["RunnablePassthrough<br/>(preserva)"]
        Fork3 --> Adjust["Ajustar tom<br/>para profissional"]
        Pass3 --> Out3["Output: {info, versao_ajustada}"]
        Adjust --> Out3
    end

    subgraph Stage4["<b>Est√°gio 4: Formata√ß√£o Final</b>"]
        Input4["Input: {info, versao_ajustada}"] --> Lambda["RunnableLambda<br/>‚Ä¢ Estrutura resultado<br/>‚Ä¢ Metadados (timestamp, count)"]
        Lambda --> Out4["Output: Resultado estruturado"]
    end

    Out1 --> Stage2
    Out2 --> Stage3
    Out3 --> Stage4
```

**C√≥digo Completo - Pipeline Multi-etapa de Tradu√ß√£o**:

```python
# src/ch04/pipeline_traducao_lcel.py
import os
from typing import Any
from dotenv import load_dotenv
from datetime import datetime
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser, JsonOutputParser
from langchain_core.runnables import (
    RunnableParallel,
    RunnablePassthrough,
    RunnableLambda,
)

# Carregar vari√°veis de ambiente (OPENAI_API_KEY, etc.)
load_dotenv()

# ============================================================================
# PARTE 1: DEFINIR OS PROMPTS (Templates das instru√ß√µes para o LLM)
# ============================================================================

# Prompt para traduzir portugu√™s ‚Üí ingl√™s
prompt_traduzir = ChatPromptTemplate.from_template(
    """Traduza o seguinte texto de portugu√™s para ingl√™s.
Mantenha o significado, tom e estilo originais.

Texto: {texto}

Tradu√ß√£o:"""
)

# Prompt para analisar caracter√≠sticas lingu√≠sticas da tradu√ß√£o
prompt_analisar = ChatPromptTemplate.from_template(
    """Analise o seguinte texto em ingl√™s e retorne um JSON com estas propriedades:
- "formality": "formal" ou "informal"
- "domain": "technical", "casual", "business", "academic" ou "general"
- "tone": "friendly", "neutral", "professional", "academic" ou "casual"
- "language_level": "beginner", "intermediate" ou "advanced"

Texto: {traducao}

Retorne APENAS o JSON, sem explica√ß√µes adicionais."""
)

# Prompt para ajustar o tom da tradu√ß√£o baseado na an√°lise
prompt_ajustar_tom = ChatPromptTemplate.from_template(
    """Baseado na an√°lise lingu√≠stica, reescreva a tradu√ß√£o em um tom mais profissional e polido.

An√°lise original:
- Formalidade: {formality}
- Dom√≠nio: {domain}
- Tom: {tone}

Tradu√ß√£o original: {traducao}

Reescreva mantendo o significado mas tornando mais apropriado para comunica√ß√£o profissional:"""
)

# ============================================================================
# PARTE 2: CONFIGURAR MODELO E PARSERS
# ============================================================================

model = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), temperature=0.7)
parser_str = StrOutputParser()
parser_json = JsonOutputParser()

# ============================================================================
# PARTE 3: DEFINIR AS FUN√á√ïES CUSTOMIZADAS (para RunnableLambda)
# ============================================================================


def formatar_saida(dados: dict) -> dict:
    """
    Formata e estrutura a sa√≠da final da pipeline.

    Recebe: Dicion√°rio complexo com dados acumulados de todos os est√°gios
    Retorna: Dicion√°rio limpo com estrutura final

    Esta fun√ß√£o demonstra como RunnableLambda permite l√≥gica Python
    customizada integrada na pipeline LCEL.
    """
    # Extrair dados do est√°gio anterior (estrutura aninhada)
    info = dados.get("info", {})
    original = info.get("original", "")
    traducao = info.get("traducao", "")
    analise = info.get("analise", {})
    versao_ajustada = dados.get("versao_ajustada", "")

    # Construir resposta estruturada
    return {
        "texto_original": original,
        "traducao_literal": traducao,
        "analise_linguistica": {
            "formalidade": analise.get("formality", "unknown"),
            "dominio": analise.get("domain", "unknown"),
            "tom": analise.get("tone", "unknown"),
            "nivel_linguagem": analise.get("language_level", "unknown"),
        },
        "versao_ajustada": versao_ajustada,
        "metadados": {
            "timestamp": datetime.now().isoformat(),
            "caracteres_original": len(original),
            "caracteres_traducao": len(traducao),
            "caracteres_ajustada": len(versao_ajustada),
        },
    }


def preservar_traducao(x: dict[str, Any]) -> dict[str, Any]:
    """
    Preserva o texto original e a tradu√ß√£o ap√≥s est√°gio 1.

    Input: Dict contendo chaves 'original' e 'traducao'
    Output: Dict com as mesmas chaves

    Esta fun√ß√£o substitui a lambda da linha 142 fornecendo type hints
    que permitem o Pylance fazer type checking apropriado.
    """
    return {"original": x["original"], "traducao": x["traducao"]}


def adicionar_analise(x: dict[str, Any]) -> dict[str, Any]:
    """
    Executa an√°lise lingu√≠stica da tradu√ß√£o e preserva dados anteriores.

    Input: Dict com chaves 'original' e 'traducao'
    Output: Dict com 'original', 'traducao' e 'analise' (resultado do est√°gio 2)

    O est√°gio 2 (an√°lise) √© invocado aqui, executando em s√©rie ap√≥s
    o est√°gio 1 completar.
    """
    return {
        "original": x["original"],
        "traducao": x["traducao"],
        "analise": estagio_2_analisar.invoke({"traducao": x["traducao"]}),
    }


def preparar_ajuste_tom(x: dict[str, Any]) -> dict[str, Any]:
    """
    Prepara dados para est√°gio 3 (ajuste de tom) mantendo estado anterior.

    Input: Dict com 'original', 'traducao' e 'analise' (contendo formality, domain, tone)
    Output: Dict com 'info' (estado anterior) e 'versao_ajustada'

    O est√°gio 3 (ajuste de tom) √© invocado aqui, usando an√°lise lingu√≠stica
    para contextualizar a reescrita profissional.
    """
    return {
        "info": x,
        "versao_ajustada": estagio_3_ajustar_tom.invoke(
            {
                "formality": x["analise"].get("formality", "formal"),
                "domain": x["analise"].get("domain", "general"),
                "tone": x["analise"].get("tone", "professional"),
                "traducao": x["traducao"],
            }
        )["versao_ajustada"],
    }


# ============================================================================
# PARTE 4: CONSTRUIR OS EST√ÅGIOS DA PIPELINE
# ============================================================================

# EST√ÅGIO 1: Tradu√ß√£o + Preserva√ß√£o do Original
estagio_1_traduzir = RunnableParallel(
    {
        "original": RunnablePassthrough(),  # Caminho A: preserva o input
        "traducao": prompt_traduzir | model | parser_str,  # Caminho B: traduz
    }
)

# EST√ÅGIO 2: An√°lise Lingu√≠stica da Tradu√ß√£o
estagio_2_analisar = prompt_analisar | model | parser_json

# EST√ÅGIO 3: Ajuste de Tom + Preserva√ß√£o de Estado
estagio_3_ajustar_tom = RunnableParallel(
    {
        "info": RunnablePassthrough(),  # Preserva: original, traducao, analise
        "versao_ajustada": prompt_ajustar_tom | model | parser_str,
    }
)

# EST√ÅGIO 4: Formata√ß√£o Final
estagio_4_formatar = RunnableLambda(formatar_saida)

# ============================================================================
# PARTE 5: MONTAR A PIPELINE LCEL COMPLETA
# ============================================================================

# Composi√ß√£o com pipe operator |
# Cada | passa o output do est√°gio anterior como input do pr√≥ximo
translation_pipeline = (
    estagio_1_traduzir  # Dict: executa paralelo (original + tradu√ß√£o)
    | RunnableLambda(preservar_traducao)  # Preserva original e tradu√ß√£o
    | RunnableLambda(adicionar_analise)  # Adiciona an√°lise lingu√≠stica
    | RunnableLambda(preparar_ajuste_tom)  # Prepara ajuste de tom
    | estagio_4_formatar  # RunnableLambda: formata sa√≠da final
)

# ============================================================================
# PARTE 6: USAR A PIPELINE
# ============================================================================

if __name__ == "__main__":
    texto_entrada = "Ol√°! Como voc√™ est√°? O clima est√° muito bonito hoje!"

    print("=" * 70)
    print("PIPELINE DE TRADU√á√ÉO MULTI-ETAPA COM LCEL")
    print("=" * 70)
    print()

    print("Processando texto...")
    print(f"Input: {texto_entrada}")
    print()

    resultado = translation_pipeline.invoke({"texto": texto_entrada})

    print("RESULTADO FINAL:")
    print("-" * 70)
    print(f"Texto Original: {resultado['texto_original']}")
    print()
    print(f"Tradu√ß√£o Literal: {resultado['traducao_literal']}")
    print()
    print("An√°lise Lingu√≠stica:")
    analise = resultado["analise_linguistica"]
    print(f"  - Formalidade: {analise['formalidade']}")
    print(f"  - Dom√≠nio: {analise['dominio']}")
    print(f"  - Tom: {analise['tom']}")
    print(f"  - N√≠vel de Linguagem: {analise['nivel_linguagem']}")
    print()
    print(f"Vers√£o Ajustada: {resultado['versao_ajustada']}")
    print()
    print("Metadados:")
    meta = resultado["metadados"]
    print(f"  - Timestamp: {meta['timestamp']}")
    print(f"  - Caracteres (original): {meta['caracteres_original']}")
    print(f"  - Caracteres (tradu√ß√£o): {meta['caracteres_traducao']}")
    print(f"  - Caracteres (ajustada): {meta['caracteres_ajustada']}")
```

**Explica√ß√£o dos 4 Est√°gios**:

**Est√°gio 1 - Tradu√ß√£o + Preserva√ß√£o**:
```python
estagio_1_traduzir = RunnableParallel(
    {
        "original": RunnablePassthrough(),  # Caminho A: preserva o input
        "traducao": prompt_traduzir | model | parser_str,  # Caminho B: traduz
    }
)
# Output: {"original": {"texto": "..."}, "traducao": "Hello! How are you?"}
```
- Cria `RunnableParallel`: executa AMBOS os caminhos simultaneamente
- `RunnablePassthrough()` mant√©m o input original intacto
- Segunda branch traduz usando prompt + model + parser
- Resultado mesclado em um dicion√°rio

**Fun√ß√µes Intermedi√°rias - O padr√£o refatorado**:

O c√≥digo refatorado encapsula cada est√°gio com fun√ß√µes Python nomeadas que invocam os est√°gios internamente:

```python
translation_pipeline = (
    estagio_1_traduzir                      # RunnableParallel
    | RunnableLambda(preservar_traducao)    # Preserva sa√≠da do est√°gio 1
    | RunnableLambda(adicionar_analise)     # Invoca EST√ÅGIO 2 aqui (linha 1915)
    | RunnableLambda(preparar_ajuste_tom)   # Invoca EST√ÅGIO 3 aqui (linhas 1931-1938)
    | estagio_4_formatar                    # Formata√ß√£o final
)
```

**Por que 3 fun√ß√µes intermedi√°rias?**
- **`preservar_traducao()`**: Type-safe wrapper que valida e estrutura output do est√°gio 1
- **`adicionar_analise()`**: Chama `estagio_2_analisar.invoke()` internamente e acumula resultado com dados anteriores
- **`preparar_ajuste_tom()`**: Chama `estagio_3_ajustar_tom.invoke()` internamente e estrutura dados para est√°gio 4

Essa abordagem traz **type safety** (type hints validados pelo Pylance) e **modularidade** sobre lambdas an√¥nimas.

**Est√°gio 2 - An√°lise Lingu√≠stica**:
```python
estagio_2_analisar = prompt_analisar | model | parser_json
```
- Pipeline que recebe `{traducao}` da fun√ß√£o intermedi√°ria `adicionar_analise()` (linha 1915)
- LLM analisa formalidade, dom√≠nio, tom e n√≠vel lingu√≠stico
- JsonOutputParser converte string JSON ‚Üí dicion√°rio Python
- **Na pipeline principal**: invocado internamente dentro de `adicionar_analise()`, n√£o diretamente

**Est√°gio 3 - Ajuste de Tom**:
```python
estagio_3_ajustar_tom = RunnableParallel(
    {
        "info": RunnablePassthrough(),
        "versao_ajustada": prompt_ajustar_tom | model | parser_str,
    }
)
```
- Pipeline que recebe dados acumulados da fun√ß√£o intermedi√°ria `preparar_ajuste_tom()`
- `RunnablePassthrough()` preserva original, tradu√ß√£o e an√°lise
- Segunda branch usa caracter√≠sticas lingu√≠sticas para reescrever em tom profissional
- **Na pipeline principal**: invocado internamente dentro de `preparar_ajuste_tom()`, n√£o diretamente

**Est√°gio 4 - Formata√ß√£o Final**:
```python
estagio_4_formatar = RunnableLambda(formatar_saida)
```
- `RunnableLambda` envolve fun√ß√£o Python `formatar_saida`
- Limpa estrutura complexa de dados acumulados
- Retorna JSON estruturado com metadados (timestamp, contagem caracteres)

**Por que este padr√£o √© poderoso**:

1. **Pipes compostos**: Cada `|` passa sa√≠da anterior ‚Üí entrada pr√≥xima
2. **Parallelismo com preserva√ß√£o**: Dict syntax executa m√∫ltiplos caminhos simultaneamente
3. **Ac√∫mulo de estado**: RunnablePassthrough mant√©m hist√≥rico atrav√©s de est√°gios
4. **Composi√ß√£o declarativa**: C√≥digo descreve FLUXO, n√£o como implementar
5. **Streaming autom√°tico**: Cada est√°gio suporta `.stream()` naturalmente
6. **Type safety com fun√ß√µes nomeadas**: Substituir lambdas por fun√ß√µes (com `dict[str, Any]` hints) permite valida√ß√£o est√°tica de tipos (Pylance) e melhor testabilidade

Este padr√£o (**compose ‚Üí preserve ‚Üí transform ‚Üí format**) √© fundamental para pipelines LCEL profissionais.

### 4.5 Composi√ß√£o Avan√ßada: LLMChain com Memory

LCEL tamb√©m pode incluir l√≥gica de mem√≥ria usando **RunnableLambda**:

```python
from langchain_core.runnables import RunnableLambda

"""
Exemplo: Pipeline com Mem√≥ria Simplificada

Este exemplo mostra como usar RunnableLambda para adicionar l√≥gica customizada
(neste caso, manuten√ß√£o de hist√≥rico) √† cadeia LCEL.

RunnableLambda transforma uma fun√ß√£o Python comum em um Runnable que:
- Integra-se naturalmente √† cadeia
- Suporta streaming automaticamente
- Pode ser testado independentemente
"""

# Mem√≥ria simplificada (em produ√ß√£o, use LangChain's memory classes)
mensagens = []

def adicionar_historico(entrada):
    """
    Fun√ß√£o customizada que adiciona entrada ao hist√≥rico.

    Recebe: string (pergunta do usu√°rio)
    Retorna: dict com hist√≥rico e entrada (para o prompt usar)

    Nota: RunnableLambda envolve esta fun√ß√£o para integr√°-la √† cadeia
    """
    mensagens.append(entrada)
    # Retorna dicion√°rio com as vari√°veis que o prompt precisa
    return {
        "historico": "\n".join(mensagens[-5:]),  # √öltimas 5 mensagens
        "entrada": entrada                        # Pergunta atual
    }

# Pipeline com mem√≥ria
# 1. RunnablePassthrough(): passa a pergunta original
# 2. RunnableLambda(adicionar_historico): fun√ß√£o customizada que mant√©m hist√≥rico
#    - Recebe a pergunta
#    - Adiciona ao hist√≥rico
#    - Retorna {"historico": "...", "entrada": "..."}
# 3. prompt: template usa {historico} e {entrada}
# 4. model: gera resposta
# 5. parser: extrai texto
chat_chain = (
    RunnablePassthrough()                        # Passa pergunta intacta
    | RunnableLambda(adicionar_historico)        # Adiciona l√≥gica customizada (mem√≥ria)
    | prompt                                      # Formata com vari√°veis
    | model                                       # LLM gera resposta
    | parser                                      # Extrai texto
)

# Testando a pipeline com mem√≥ria
resposta1 = chat_chain.invoke("Ol√°, qual √© seu nome?")
print(f"Resposta 1: {resposta1}")

# Agora, adicionar_historico ter√° a pergunta anterior no hist√≥rico
resposta2 = chat_chain.invoke("O que voc√™ acabou de me contar?")
print(f"Resposta 2: {resposta2}")

# Observa√ß√£o: O hist√≥rico agora cont√©m ambas as mensagens
print(f"Hist√≥rico completo: {mensagens}")
```

**Por que RunnableLambda √© poderoso aqui?**
- Transforma uma fun√ß√£o Python em um Runnable
- Integra-se perfeitamente com LCEL
- Suporta automaticamente `.stream()`, `.batch()`, opera√ß√µes ass√≠ncronas
- Sem precisar de classes ou c√≥digo boilerplate

### 4.6 Por Que LCEL N√£o √â Suficiente: Motiva√ß√£o para LangGraph

LCEL √© excelente para **pipelines determin√≠sticas lineares**, mas tem limita√ß√µes importantes quando voc√™ precisa de l√≥gica mais complexa:

| Caracter√≠stica | LCEL | LangGraph |
|---|---|---|
| **Pipes lineares** | ‚úÖ Perfeito | ‚úÖ Perfeito |
| **Streaming** | ‚úÖ Autom√°tico | ‚úÖ Autom√°tico |
| **Loops/Itera√ß√µes** | ‚ùå Imposs√≠vel | ‚úÖ Nativo |
| **Decis√µes condicionais** | ‚ùå Imposs√≠vel | ‚úÖ Nativo |
| **Estado complexo (TypedDict)** | ‚ùå Apenas vari√°veis simples | ‚úÖ Estados tipados |
| **Persist√™ncia/Checkpointing** | ‚ùå N√£o | ‚úÖ Sim |
| **Human-in-the-Loop** | ‚ùå N√£o | ‚úÖ Sim (pausar, retomar) |
| **Multi-agente / paraleliza√ß√£o** | ‚ùå N√£o (apenas serial) | ‚úÖ Sim |

**Exemplo pr√°tico de limita√ß√£o**: Um agente ReAct que faz o ciclo:
1. **Think** (raciocina)
2. **Act** (executa a√ß√£o/tool)
3. **Observe** (observa resultado)
4. **Decide** (continua ou para?)

Este ciclo √© **imposs√≠vel em LCEL puro** porque n√£o h√° suporte para loops. √â aqui que **LangGraph** entra, permitindo estados, n√≥s e arestas para representar fluxos complexos.

> **üìö Refer√™ncia**: O Cap√≠tulo 6 (LangGraph) explora como construir agents e workflows com loops e decis√µes condicionais.

### 4.7 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- O que √© **LCEL** e por que simplifica composi√ß√£o
- Como usar o operador **pipe (`|`)** para encadear componentes
- Construir uma **pipeline de tradu√ß√£o multi-etapa com LCEL**
- Limitar do LCEL (loops, decis√µes complexas)
- **Pr√≥ximo passo**: LangGraph para agentes com ciclos

### 4.8 Exerc√≠cios

1. **Modifique a pipeline de tradu√ß√£o**: Adicione um est√°gio de "back-translation" que traduz a vers√£o em ingl√™s de volta para portugu√™s para verificar a qualidade da tradu√ß√£o.
   - Dica: Use `RunnableLambda` para comparar a tradu√ß√£o reversa com o original
   - Calcule um score de qualidade baseado na similaridade

2. **Reuse de chains**: Crie uma fun√ß√£o que retorna chains reutiliz√°veis para diferentes idiomas (PT‚ÜíEN, PT‚ÜíES, PT‚ÜíFR).
   - Dica: Parametrize o idioma alvo e crie factory functions

3. **Streaming**: Implemente um exemplo que usa LCEL com streaming de tokens incrementais da tradu√ß√£o.
   - Use `.stream()` em vez de `.invoke()`
   - Veja tokens aparecerem em tempo real

---

## Cap√≠tulo 5: Modularidade e Interoperabilidade

### 5.1 Separa√ß√£o de Pacotes no LangChain v1.0

Um grande problema das vers√µes anteriores era: "Para usar OpenAI, instalo `langchain` e `openai`?". A resposta era confusa.

**LangChain v1.0+** resolve isso com uma arquitetura modular clara:

```
langchain-core
‚îú‚îÄ‚îÄ Tipos, mensagens, LCEL
‚îî‚îÄ‚îÄ Interfaces abstratas (LLM, ChatModel, Tool, etc.)

langchain
‚îú‚îÄ‚îÄ Constru√ß√µes de alto n√≠vel
‚îî‚îÄ‚îÄ Abstra√ß√µes agn√≥sticas

langchain-openai
‚îú‚îÄ‚îÄ ChatOpenAI, OpenAIEmbeddings
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de OpenAI

langchain-anthropic
‚îú‚îÄ‚îÄ ChatAnthropic
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de Anthropic

langchain-google-genai
‚îú‚îÄ‚îÄ ChatGoogleGenerativeAI
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de Google

langchain-community
‚îú‚îÄ‚îÄ Conectores mantidos pela comunidade
‚îî‚îÄ‚îÄ Integra√ß√µes experimentais
```

**Benef√≠cio**: Voc√™ instala apenas o que precisa.

```bash
# Uso com OpenAI
pip install langchain langchain-core langchain-openai

# Trocar para Anthropic (sem quebrar seu c√≥digo LCEL)
pip uninstall langchain-openai
pip install langchain-anthropic
# S√≥ troca a importa√ß√£o: ChatOpenAI ‚Üí ChatAnthropic
```

### 5.2 Standard Content Blocks (Interoperabilidade de Output)

Diferentes provedores retornam respostas em formatos diferentes. **Standard Content Blocks** (v1.0) normalizam isso.

Uma mensagem agora pode conter m√∫ltiplos blocos estruturados:

```python
from langchain_core.messages import AIMessage

# Output normalizado (funciona com OpenAI, Claude, Gemini)
message = AIMessage(
    content="Aqui est√° a resposta",
    content_blocks=[
        {"type": "text", "text": "Resposta principal"},
        {"type": "reasoning", "text": "Meu racioc√≠nio..."},
        {"type": "tool_call", "tool": "search", "args": {...}},
        {"type": "citation", "source": "documento_1.pdf"}
    ]
)
```

Isso permite que **uma ferramenta de UI** renderize respostas de qualquer modelo sem mudan√ßas:

```python
def renderizar_resposta(message: AIMessage):
    """Renderiza qualquer mensagem de qualquer provedor igual."""
    for block in message.content_blocks:
        if block["type"] == "text":
            print(block["text"])
        elif block["type"] == "reasoning":
            print(f"[Racioc√≠nio] {block['text']}")
        elif block["type"] == "tool_call":
            print(f"[Tool] {block['tool']}")
```

### 5.3 Portabilidade: Trocar Provedores sem Refatora√ß√£o

Gra√ßas √† separa√ß√£o de pacotes e Standard Content Blocks, seu c√≥digo se torna **agn√≥stico ao provedor**:

```python
# /src/ch05/config.py
# /src/ch05/config.py
import os
from dotenv import load_dotenv
from langchain_core.language_models.chat_models import BaseChatModel

load_dotenv()


def get_model(provider: str = "") -> BaseChatModel:
    """Factory que retorna o modelo configurado."""
    provider = provider or os.getenv("LLM_PROVIDER", "openai")

    if provider == "openai":
        from langchain_openai import ChatOpenAI

        modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), temperature=0
        )
        return modelo

    elif provider == "anthropic":
        from langchain_anthropic import ChatAnthropic

        modelo = ChatAnthropic(
            model_name=os.getenv("ANTHROPIC_MODEL", "claude-3-5-sonnet"),
            timeout=60,
            stop=["\n"],
        )
        return modelo

    elif provider == "google":
        from langchain_google_genai import ChatGoogleGenerativeAI

        modelo = ChatGoogleGenerativeAI(
            model=os.getenv("GOOGLE_MODEL", "gemini-2.5-flush-lite")
        )
        return modelo

    else:
        raise ValueError(f"Provider {provider} n√£o suportado")
```

**Uso**:

```bash
# Usar OpenAI
LLM_PROVIDER=openai python seu_app.py

# Trocar para Anthropic (sem mexer no c√≥digo!)
LLM_PROVIDER=anthropic python seu_app.py

# Trocar para Google (sem mexer no c√≥digo!)
LLM_PROVIDER=google python seu_app.py
```

### 5.4 Exemplo: Pipeline Multi-Provider

Construa um pipeline que usa m√∫ltiplos provedores:

```python
# /src/ch05/pipeline_multi_provider.py
from config import get_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

# Prompts espec√≠ficos para cada tarefa
analise_prompt = ChatPromptTemplate.from_template(
    "Analise criticamente: {texto}"
)

review_prompt = ChatPromptTemplate.from_template(
    "Revise e aprove ou critique: {analise}"
)

# Modelos: use diferentes provedores para diferentes tarefas
modelo_analista = get_model("openai")
modelo_critico = get_model("google")

# Pipeline
pipeline = (
    {"texto": RunnablePassthrough()}
    | analise_prompt
    | modelo_analista
    | StrOutputParser()
    | {"analise": RunnablePassthrough()}
    | review_prompt
    | modelo_critico
    | StrOutputParser()
)

resultado = pipeline.invoke("Escreva uma proposta de neg√≥cio")
print(f"Resultado final:\n{resultado}")
```

### 5.5 Boas Pr√°ticas: Depend√™ncias M√≠nimas

Ao desenvolver bibliotecas ou aplica√ß√µes, siga este padr√£o:

```python
# Seu pacote: requirements.txt
langchain-core>=1.0.0  # Apenas abstra√ß√µes
pydantic>=2.0

# Seu c√≥digo: suporta m√∫ltiplos provedores
def processar_com_llm(model: Optional[LLM] = None):
    """
    Se model √© None, usa OpenAI. Caso contr√°rio, usa o model passado.
    Assim, o usu√°rio pode injetar qualquer modelo.
    """
    if model is None:
        from langchain_openai import ChatOpenAI
        model = ChatOpenAI()

    # Use model de forma agn√≥stica
    return model.invoke(...)
```

### 5.6 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- A **arquitetura modular** do LangChain v1.0+
- Como **separar pacotes** por provedor
- **Standard Content Blocks** para interoperabilidade
- Como construir c√≥digo **agn√≥stico ao provedor**
- Boas pr√°ticas para **depend√™ncias m√≠nimas**

### 5.7 Exerc√≠cios

1. **Factory pattern**: Crie uma classe `LLMFactory` que instancia modelos baseado em vari√°veis de ambiente.

2. **Multi-provider pipeline**: Construa um pipeline que usa OpenAI para rascunho e Anthropic para revis√£o.

3. **Teste de portabilidade**: Implemente um teste que roda o mesmo c√≥digo com 3 provedores diferentes.

---

# PARTE III: INTRODU√á√ÉO AO LANGGRAPH

---

## Cap√≠tulo 6: Introdu√ß√£o ao LangGraph v1.0+

### 6.1 De Cadeias Lineares a Grafos C√≠clicos

No Cap√≠tulo 5, vimos como compor pipelines com LCEL. Mas LCEL tem limita√ß√µes: sem loops, sem decis√µes condicionais complexas. Precisamos de mais poder.

**LangGraph v1.0+** fornece exatamente isso: grafos de estado para orquestra√ß√£o complexa.

Antes de entrar em detalhes, vamos entender o problema:

No Cap√≠tulo 3, vimos como executar tools manualmente com um loop `while`. Esse c√≥digo funciona, mas rapidamente se torna complexo quando precisamos de:

- M√∫ltiplos caminhos de decis√£o
- Loops condicionais
- Persist√™ncia de estado
- Human-in-the-loop

**LangGraph v1.0+** resolve esses problemas modelando o fluxo como um **grafo de estado**:

```mermaid
flowchart LR
    START[("START")] --> LLMCall["<b>llm_call</b>"]
    LLMCall --> Decision{"<b>should_continue</b>"}
    Decision -->|SIM| ToolNode["<b>tool_node</b>"]
    Decision -->|N√ÉO| END[("END")]
    ToolNode --> LLMCall
```

### 6.2 Conceito de Estado (TypedDict com Annotated)

O **estado** √© o cora√ß√£o do LangGraph. Ele armazena todas as informa√ß√µes que fluem pelo grafo:

```python
from typing import TypedDict, Annotated
from langchain_core.messages import AnyMessage
import operator

class AgentState(TypedDict):
    # Lista de mensagens - usa reducer para acumular
    messages: Annotated[list[AnyMessage], operator.add]

    # Outros campos s√£o sobrescritos por padr√£o
    contador: int
    usuario_nome: str
```

#### O Reducer `operator.add`

O `Annotated[..., operator.add]` √© crucial para entender o LangGraph:

- **Sem reducer**: Novo valor **substitui** o anterior
- **Com `operator.add`**: Novo valor √© **concatenado** ao anterior

```python
# SEM reducer (substitui)
estado = {"contador": 1}
atualiza√ß√£o = {"contador": 5}
# Resultado: {"contador": 5}

# COM operator.add (acumula)
estado = {"messages": [msg1]}
atualiza√ß√£o = {"messages": [msg2]}
# Resultado: {"messages": [msg1, msg2]}
```

Isso permite que m√∫ltiplos n√≥s contribuam para o hist√≥rico de mensagens sem sobrescrever uns aos outros.

### 6.3 N√≥s (Nodes): Unidades de Computa√ß√£o

Um **n√≥** √© uma fun√ß√£o Python que:
1. Recebe o estado atual
2. Processa/modifica dados
3. Retorna uma **atualiza√ß√£o parcial** do estado

```python
def meu_no(state: AgentState) -> dict:
    """N√≥ que processa o estado."""
    # Ler do estado
    mensagens = state["messages"]
    contador = state["contador"]

    # Processar...
    novo_contador = contador + 1

    # Retornar APENAS o que mudou
    return {
        "contador": novo_contador,
        "messages": [AIMessage(content="Processado!")]
    }
```

### 6.4 Arestas (Edges): Definindo o Fluxo

Existem dois tipos de arestas:

#### Arestas Normais (Est√°ticas)

Sempre v√£o para o mesmo destino:

```python
# Ap√≥s 'n√≥_a', sempre execute 'n√≥_b'
grafo.add_edge("n√≥_a", "n√≥_b")
```

#### Arestas Condicionais

Usam uma fun√ß√£o para decidir o pr√≥ximo n√≥:

```python
from typing import Literal

def decidir_proximo(state: AgentState) -> Literal["processar", "finalizar"]:
    """Decide qual n√≥ executar baseado no estado."""
    if state["contador"] > 5:
        return "finalizar"
    return "processar"

grafo.add_conditional_edges(
    "avaliar",           # N√≥ de origem
    decidir_proximo,     # Fun√ß√£o de decis√£o
    {                    # Mapeamento de retorno -> n√≥ destino
        "processar": "processar_node",
        "finalizar": END
    }
)
```

### 6.5 Compila√ß√£o do Grafo

Ap√≥s definir estado, n√≥s e arestas, compilamos o grafo:

```python
from langgraph.graph import StateGraph, START, END

# 1. Criar grafo tipado
grafo = StateGraph(AgentState)

# 2. Adicionar n√≥s
grafo.add_node("processar", funcao_processar)
grafo.add_node("decidir", funcao_decidir)

# 3. Adicionar arestas
grafo.add_edge(START, "processar")
grafo.add_conditional_edges("processar", funcao_decisao)
grafo.add_edge("decidir", END)

# 4. Compilar
app = grafo.compile()

# 5. Executar
resultado = app.invoke({"messages": [], "contador": 0})
```

### 6.6 Exemplo Pr√°tico: Grafo Simples

Vamos criar um grafo que conta at√© 3:

```python
# /src/ch06/grafo_contador.py
from typing import TypedDict, Literal
from langgraph.graph import StateGraph, START, END

# 1. Definir estado
class ContadorState(TypedDict):
    valor: int
    historico: list[str]

# 2. Definir n√≥s
def incrementar(state: ContadorState) -> dict:
    """Incrementa o contador."""
    novo_valor = state["valor"] + 1
    return {
        "valor": novo_valor,
        "historico": state["historico"] + [f"Incrementado para {novo_valor}"]
    }

def verificar(state: ContadorState) -> dict:
    """Apenas passa pelo n√≥ de verifica√ß√£o."""
    return {"historico": state["historico"] + ["Verificando..."]}

# 3. Fun√ß√£o de decis√£o
def deve_continuar(state: ContadorState) -> Literal["continuar", "parar"]:
    if state["valor"] < 3:
        return "continuar"
    return "parar"

# 4. Construir grafo
grafo = StateGraph(ContadorState)

# Adicionar n√≥s
grafo.add_node("incrementar", incrementar)
grafo.add_node("verificar", verificar)

# Adicionar arestas
grafo.add_edge(START, "incrementar")
grafo.add_edge("incrementar", "verificar")
grafo.add_conditional_edges(
    "verificar",
    deve_continuar,
    {
        "continuar": "incrementar",  # Loop!
        "parar": END
    }
)

# 5. Compilar
app = grafo.compile()

# 6. Executar
estado_inicial = {"valor": 0, "historico": ["In√≠cio"]}
resultado = app.invoke(estado_inicial)

print(f"Valor final: {resultado['valor']}")
print("Hist√≥rico:")
for item in resultado["historico"]:
    print(f"  - {item}")
```

**Sa√≠da**:
```
Valor final: 3
Hist√≥rico:
  - In√≠cio
  - Incrementado para 1
  - Verificando...
  - Incrementado para 2
  - Verificando...
  - Incrementado para 3
  - Verificando...
```

### 6.7 Conceitos v1.0+: Execu√ß√£o Dur√°vel e Super-Steps

#### Super-Steps

Cada "rodada" de execu√ß√£o do grafo √© um **super-step**. O LangGraph salva o estado ap√≥s cada super-step, permitindo:

- **Recupera√ß√£o de falhas**: Se o servidor cair, retoma do √∫ltimo ponto salvo
- **Time-travel**: Voltar a estados anteriores para depura√ß√£o
- **Human-in-the-loop**: Pausar execu√ß√£o para aprova√ß√£o humana

#### Execu√ß√£o Dur√°vel

Com um **checkpointer**, o LangGraph persiste o estado:

```python
from langgraph.checkpoint.memory import MemorySaver

# Criar checkpointer
checkpointer = MemorySaver()

# Compilar com checkpointer
app = grafo.compile(checkpointer=checkpointer)

# Executar com thread_id
config = {"configurable": {"thread_id": "sessao-123"}}
resultado = app.invoke(estado_inicial, config=config)
```

### 6.8 Exemplo Completo: Grafo com LLM

Vamos criar um grafo que usa um LLM para responder perguntas:

```python
# /src/ch06/grafo_llm.py
import os
from typing import TypedDict, Annotated, Literal
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage, AnyMessage
from langgraph.graph import StateGraph, START, END
import operator

load_dotenv()

# === ESTADO ===
class ChatState(TypedDict):
    messages: Annotated[list[AnyMessage], operator.add]

# === MODELO ===
modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

# === N√ìS ===
def preparar(state: ChatState) -> dict:
    """Adiciona system message se necess√°rio."""
    messages = state["messages"]

    # Se n√£o h√° system message, adicionar
    if not messages or not isinstance(messages[0], SystemMessage):
        system = SystemMessage(content="Voc√™ √© um assistente prestativo. Responda em portugu√™s.")
        return {"messages": [system]}

    return {"messages": []}

def chamar_modelo(state: ChatState) -> dict:
    """Chama o LLM."""
    resposta = modelo.invoke(state["messages"])
    return {"messages": [resposta]}

# === GRAFO ===
grafo = StateGraph(ChatState)

# N√≥s
grafo.add_node("preparar", preparar)
grafo.add_node("chamar_modelo", chamar_modelo)

# Arestas
grafo.add_edge(START, "preparar")
grafo.add_edge("preparar", "chamar_modelo")
grafo.add_edge("chamar_modelo", END)

# Compilar
app = grafo.compile()

# === USAR ===
def chat(mensagem: str) -> str:
    resultado = app.invoke({
        "messages": [HumanMessage(content=mensagem)]
    })
    return resultado["messages"][-1].content

# Testar
if __name__ == "__main__":
    print(chat("Qual √© a capital da Fran√ßa?"))
    print("\n---\n")
    print(chat("E do Brasil?"))
```

### 6.9 Visualizando o Grafo

O LangGraph permite visualizar a estrutura do grafo:

```python
# Requer: pip install pygraphviz ou usar Mermaid

# Visualizar como Mermaid (pode copiar para mermaid.live)
print(app.get_graph().draw_mermaid())

# Ou salvar como imagem (requer pygraphviz)
# app.get_graph().draw_png("grafo.png")
```

**Sa√≠da Mermaid**:
```mermaid
graph TD
    __start__ --> preparar
    preparar --> chamar_modelo
    chamar_modelo --> __end__
```

### 6.10 Anatomia de um Agente ReAct com LangGraph v1.0+

#### 6.10.1 O Padr√£o ReAct: Raciocinar ‚Üí Agir ‚Üí Observar

**ReAct** (Reasoning + Acting) √© o padr√£o mais comum para construir agentes. O ciclo √©:

1. **Raciocinar**: O LLM analisa a situa√ß√£o e decide o que fazer
2. **Agir**: Se necess√°rio, chama uma tool
3. **Observar**: Recebe o resultado da tool
4. **Repetir**: Volta ao passo 1 at√© ter uma resposta final

```mermaid
flowchart TD
    Reason["<b>Raciocinar</b><br/>(LLM)"] --> Act["<b>Agir</b><br/>(Tool)"]
    Act --> Observe["<b>Observar</b><br/>(Resultado)"]
    Reason --> Decision{"<b>Tem<br/>resposta?</b>"}
    Observe --> Decision
    Decision -->|N√£o| Reason
    Decision -->|Sim| Final["<b>Resposta Final</b>"]
```

#### 6.10.2 Estrutura do Agente ReAct no LangGraph

Um agente ReAct no LangGraph v1.0+ tem:

1. **Estado** (`AgentState`): Armazena mensagens
2. **N√≥ LLM** (`llm_call`): Chama o modelo com tools bindadas
3. **N√≥ Tool** (`tool_node`): Executa tools chamadas
4. **Aresta Condicional** (`should_continue`): Decide se continua ou termina

#### 6.10.3 Implementa√ß√£o Completa: Agente ReAct Multi-Funcional

Vamos construir um agente ReAct com m√∫ltiplas ferramentas. Use as fun√ß√µes de padr√£o (`llm_call`, `tool_node`, `should_continue`) da se√ß√£o **Padr√µes Reutiliz√°veis** (com adapta√ß√µes conforme necess√°rio para seu caso de uso):

```python
# /src/ch06/agente_react_completo.py
import os
import operator
from typing import TypedDict, Annotated, Literal, Optional
from datetime import datetime
from dotenv import load_dotenv

from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage, AnyMessage
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from langgraph.graph import StateGraph, START, END
from langgraph.config import get_config

load_dotenv()

# === CONFIGURA√á√ÉO INICIAL ===
TAREFAS_DB: dict[int, list[dict]] = {
    1: [
        {"id": 1, "titulo": "Estudar Python", "estado": "pendente", "vencimento": "2025-12-15"},
        {"id": 2, "titulo": "Fazer compras", "estado": "concluida", "vencimento": "2025-12-10"},
    ]
}
NEXT_ID = 3

# === FERRAMENTAS (TOOLS) ===

@tool
def calcular(operacao: str, a: float, b: float) -> str:
    """Calcula opera√ß√µes matem√°ticas b√°sicas.

    Args:
        operacao: 'somar', 'subtrair', 'multiplicar', 'dividir'
        a: Primeiro n√∫mero
        b: Segundo n√∫mero
    """
    operacoes = {
        "somar": lambda x, y: x + y,
        "subtrair": lambda x, y: x - y,
        "multiplicar": lambda x, y: x * y,
        "dividir": lambda x, y: x / y if y != 0 else None,
    }

    if operacao not in operacoes:
        return f"Opera√ß√£o '{operacao}' n√£o reconhecida."

    try:
        resultado = operacoes[operacao](a, b)
        if resultado is None:
            return "Erro: divis√£o por zero."
        return f"O resultado de {a} {operacao} {b} √© {resultado}"
    except Exception as e:
        return f"Erro ao calcular: {e}"

@tool
def obter_hora() -> str:
    """Obt√©m a hora atual do sistema."""
    agora = datetime.now()
    return f"Agora s√£o {agora.strftime('%H:%M:%S')} do dia {agora.strftime('%d/%m/%Y')}"

@tool
def listar_tarefas(estado: Optional[str] = None) -> str:
    """Lista as tarefas do usu√°rio.

    Args:
        estado: Filtrar por estado (pendente, concluida). Deixe vazio para todas.
    """
    config = get_config()
    usuario_id = config.get("configurable", {}).get("usuario_id", 1)
    tarefas = TAREFAS_DB.get(usuario_id, [])

    if estado:
        tarefas = [t for t in tarefas if t["estado"] == estado]

    if not tarefas:
        return "Nenhuma tarefa encontrada."

    resultado = f"Encontradas {len(tarefas)} tarefa(s):\n"
    for t in tarefas:
        emoji = "‚è≥" if t["estado"] == "pendente" else "‚úÖ"
        resultado += f"\n{emoji} [{t['id']}] {t['titulo']}"
        if t.get("vencimento"):
            resultado += f" (vence: {t['vencimento']})"

    return resultado

class CriarTarefaInput(BaseModel):
    titulo: str = Field(description="T√≠tulo da tarefa")
    vencimento: Optional[str] = Field(default=None, description="Data de vencimento (YYYY-MM-DD)")

@tool(args_schema=CriarTarefaInput)
def criar_tarefa(titulo: str, vencimento: Optional[str] = None) -> str:
    """Cria uma nova tarefa."""
    global NEXT_ID
    config = get_config()
    usuario_id = config.get("configurable", {}).get("usuario_id", 1)

    if usuario_id not in TAREFAS_DB:
        TAREFAS_DB[usuario_id] = []

    nova_tarefa = {
        "id": NEXT_ID,
        "titulo": titulo,
        "estado": "pendente",
        "vencimento": vencimento
    }
    TAREFAS_DB[usuario_id].append(nova_tarefa)
    NEXT_ID += 1

    return f"Tarefa criada com sucesso! ID: {nova_tarefa['id']}, T√≠tulo: {titulo}"

@tool
def concluir_tarefa(tarefa_id: int) -> str:
    """Marca uma tarefa como conclu√≠da."""
    config = get_config()
    usuario_id = config.get("configurable", {}).get("usuario_id", 1)
    tarefas = TAREFAS_DB.get(usuario_id, [])

    for tarefa in tarefas:
        if tarefa["id"] == tarefa_id:
            tarefa["estado"] = "concluida"
            return f"Tarefa '{tarefa['titulo']}' marcada como conclu√≠da!"

    return f"Tarefa com ID {tarefa_id} n√£o encontrada."

ALL_TOOLS = [calcular, obter_hora, listar_tarefas, criar_tarefa, concluir_tarefa]
TOOLS_BY_NAME = {t.name: t for t in ALL_TOOLS}

# === DEFINIR ESTADO ===
class AgentState(TypedDict):
    messages: Annotated[list[AnyMessage], operator.add]

# === CONFIGURAR MODELO ===
SYSTEM_PROMPT = """Voc√™ √© um assistente inteligente com acesso a ferramentas.

## Ferramentas Dispon√≠veis
- calcular: Para fazer c√°lculos matem√°ticos (somar, subtrair, multiplicar, dividir)
- obter_hora: Para saber a hora atual
- listar_tarefas: Para listar tarefas (opcionalmente filtradas por estado)
- criar_tarefa: Para criar novas tarefas
- concluir_tarefa: Para marcar tarefas como conclu√≠das

## Instru√ß√µes
- Responda sempre em portugu√™s brasileiro
- Use as ferramentas quando necess√°rio
- Seja conciso e direto nas respostas
- Para datas, use formato DD/MM/AAAA
"""

modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"), temperature=0)
modelo_com_tools = modelo.bind_tools(ALL_TOOLS)

# === N√ìS DO GRAFO ===
# Refer√™ncia: se√ß√£o "Padr√µes Reutiliz√°veis"

def llm_call(state: AgentState) -> dict:
    """N√≥ que chama o LLM."""
    messages = state["messages"]
    if not messages or not isinstance(messages[0], SystemMessage):
        messages = [SystemMessage(content=SYSTEM_PROMPT)] + messages
    response = modelo_com_tools.invoke(messages)
    return {"messages": [response]}

def tool_node(state: AgentState) -> dict:
    """N√≥ que executa tools."""
    messages = state["messages"]
    last_message = messages[-1]
    tool_messages = []

    for tool_call in last_message.tool_calls:
        try:
            result = TOOLS_BY_NAME[tool_call["name"]].invoke(tool_call["args"])
        except Exception as e:
            result = f"Erro ao executar {tool_call['name']}: {e}"

        tool_messages.append(ToolMessage(
            content=str(result),
            tool_call_id=tool_call["id"]
        ))

    return {"messages": tool_messages}

def should_continue(state: AgentState) -> Literal["tool_node", "__end__"]:
    """Fun√ß√£o de decis√£o."""
    messages = state["messages"]
    last_message = messages[-1]

    if hasattr(last_message, "tool_calls") and last_message.tool_calls:
        return "tool_node"

    return "__end__"

# === CONSTRUIR E COMPILAR O GRAFO ===
def create_agent():
    graph = StateGraph(AgentState)
    graph.add_node("llm_call", llm_call)
    graph.add_node("tool_node", tool_node)
    graph.add_edge(START, "llm_call")
    graph.add_conditional_edges(
        "llm_call",
        should_continue,
        {"tool_node": "tool_node", "__end__": END}
    )
    graph.add_edge("tool_node", "llm_call")
    return graph.compile()

# === TESTAR O AGENTE ===
def main():
    agent = create_agent()
    usuario_id = 1

    print("=== Agente ReAct Multi-Funcional ===")
    print("Digite 'sair' para encerrar.\n")

    while True:
        entrada = input("Voc√™: ").strip()
        if entrada.lower() == "sair":
            break
        if not entrada:
            continue

        resultado = agent.invoke(
            {"messages": [HumanMessage(content=entrada)]},
            config={"configurable": {"usuario_id": usuario_id}}
        )

        print(f"Agente: {resultado['messages'][-1].content}\n")

if __name__ == "__main__":
    main()
```

#### 6.10.4 Testando e Entendendo o Fluxo

**Exemplos de uso:**

```
Voc√™: Quanto √© 25 vezes 4?
Agente: 25 vezes 4 √© igual a 100.

Voc√™: Que horas s√£o?
Agente: Agora s√£o 15:30:45 do dia 10/12/2025.

Voc√™: Liste minhas tarefas pendentes
Agente: Encontradas 1 tarefa(s):
‚è≥ [1] Estudar Python (vence: 2025-12-15)

Voc√™: Crie uma tarefa "Aprender LangGraph" com vencimento em 2025-12-20
Agente: Tarefa criada com sucesso! ID: 3, T√≠tulo: Aprender LangGraph

Voc√™: Marque a tarefa 2 como conclu√≠da
Agente: Tarefa 'Fazer compras' marcada como conclu√≠da!
```

**Fluxo de Execu√ß√£o (mapa mental):**

```mermaid
graph LR
    START --> llm["llm_call"]
    llm --> check{"should_continue"}
    check -->|tem tool_calls| tool["tool_node"]
    check -->|sem tool_calls| END
    tool --> llm
```

Para entender melhor as fun√ß√µes `llm_call`, `tool_node` e `should_continue`, consulte a se√ß√£o **Padr√µes Reutiliz√°veis**.

#### 6.10.5 Alternativa: create_react_agent() do LangGraph

Para casos simples, use a fun√ß√£o helper do LangGraph:

```python
# /src/ch06/usando_create_react_agent.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langgraph.prebuilt import create_react_agent

load_dotenv()

@tool
def somar(a: int, b: int) -> int:
    """Soma dois n√∫meros."""
    return a + b

@tool
def multiplicar(a: int, b: int) -> int:
    """Multiplica dois n√∫meros."""
    return a * b

modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

# Criar agente com UMA linha!
agent = create_react_agent(
    model=modelo,
    tools=[somar, multiplicar],
    prompt="Voc√™ √© uma calculadora. Responda em portugu√™s."
)

resultado = agent.invoke({
    "messages": [("human", "Quanto √© 10 + 5?")]
})

print(resultado["messages"][-1].content)
```

> **Quando usar cada abordagem?**
> - `create_react_agent()`: Prototipagem r√°pida, casos simples, PoC
> - Grafo manual: Controle total, l√≥gica customizada, debug, produ√ß√£o

#### 6.10.6 Adicionando Contexto do Usu√°rio

Em aplica√ß√µes reais, as tools precisam saber **quem** est√° usando. Use `get_config()`:

```python
# /src/ch06/tools_com_contexto.py
from langchain_core.tools import tool
from langgraph.config import get_config

@tool
def listar_minhas_tarefas() -> str:
    """Lista as tarefas do usu√°rio atual."""
    config = get_config()
    usuario_id = config.get("configurable", {}).get("usuario_id")

    if not usuario_id:
        return "Erro: usu√°rio n√£o identificado"

    tarefas = buscar_tarefas_do_banco(usuario_id)
    return formatar_tarefas(tarefas)

# Ao invocar, passar o contexto:
resultado = agent.invoke(
    {"messages": [HumanMessage(content="Liste minhas tarefas")]},
    config={"configurable": {"usuario_id": 123}}
)
```

#### 6.10.7 Exerc√≠cios

1. **Adicione uma tool de exclus√£o**: Implemente `excluir_tarefa(tarefa_id)` no agente.

2. **Limite de itera√ß√µes**: Modifique o agente para parar ap√≥s 5 chamadas de tools (evitar loops infinitos).

3. **M√∫ltiplas tools em sequ√™ncia**: Teste o agente com "Crie uma tarefa 'Estudar' e depois liste todas".

4. **Contexto de usu√°rio**: Modifique o exemplo para suportar m√∫ltiplos usu√°rios com `config={"configurable": {"usuario_id": X}}`.

### 6.11 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu os **fundamentos do LangGraph v1.0+**:

- A diferen√ßa entre **cadeias lineares** (LCEL) e **grafos c√≠clicos** (LangGraph)
- Como definir **estado** com `TypedDict` e `Annotated`
- O papel do **reducer** `operator.add` para acumular dados
- Como criar **n√≥s** que processam estado
- Como definir **arestas** normais e condicionais
- Como **compilar** e **executar** um grafo
- O padr√£o **ReAct**: Raciocinar ‚Üí Agir ‚Üí Observar ‚Üí Repetir
- Implementa√ß√£o de agentes com **llm_call** e **tool_node**
- A fun√ß√£o **should_continue** para decis√µes de fluxo
- Tools com contexto via **get_config()**
- A fun√ß√£o helper **create_react_agent()** do LangGraph
