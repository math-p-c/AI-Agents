# Tutorial Completo de LangChain e LangGraph v1.0+

**Desenvolvimento de Agentes Inteligentes com Python**

---

## Sum√°rio

### Parte I: Fundamentos
1. [Introdu√ß√£o ao Ecossistema LangChain](#cap√≠tulo-1-introdu√ß√£o-ao-ecossistema-langchain)
2. [Mensagens e Prompts](#cap√≠tulo-2-mensagens-e-prompts)
3. [Tools (Ferramentas)](#cap√≠tulo-3-tools-ferramentas)

### Parte II: LCEL e Composi√ß√£o Declarativa
4. [LangChain Expression Language (LCEL)](#cap√≠tulo-4-langchain-expression-language-lcel)
5. [Modularidade e Interoperabilidade](#cap√≠tulo-5-modularidade-e-interoperabilidade)

---

# PARTE I: FUNDAMENTOS

---

## Cap√≠tulo 1: Introdu√ß√£o ao Ecossistema LangChain

### 1.1 O Que S√£o LLMs e Por Que Precisamos de Frameworks?

**Large Language Models (LLMs)** s√£o modelos de intelig√™ncia artificial treinados em vastas quantidades de texto para compreender e gerar linguagem natural. Exemplos incluem GPT-4, Claude, Llama e Gemini.

Embora as APIs desses modelos sejam poderosas, construir aplica√ß√µes robustas diretamente sobre elas apresenta desafios:

- **Gerenciamento de conversas**: Manter hist√≥rico, contexto e estado
- **Integra√ß√£o com ferramentas**: Permitir que o modelo execute a√ß√µes no mundo real
- **Tratamento de erros**: Lidar com falhas, timeouts e respostas inesperadas
- **Orquestra√ß√£o complexa**: Coordenar m√∫ltiplas chamadas e decis√µes

√â aqui que frameworks como **LangChain** e **LangGraph** entram em cena.

### 1.2 LangChain vs LangGraph: Qual a Diferen√ßa?

| Aspecto | LangChain | LangGraph |
|---------|-----------|-----------|
| **Foco** | Componentes e primitivos | Orquestra√ß√£o e fluxo |
| **Abstra√ß√£o** | Mensagens, prompts, tools | Grafos de estado, n√≥s, arestas |
| **Uso** | Blocos de constru√ß√£o | Coordena√ß√£o de agentes |
| **Analogia** | "Pe√ßas de Lego" | "Manual de montagem" |

**LangChain v1.0+** fornece os componentes fundamentais:
- `ChatOpenAI`, `ChatAnthropic` - Interfaces para LLMs
- `SystemMessage`, `HumanMessage`, `AIMessage` - Tipos de mensagens
- `@tool` - Decorator para criar ferramentas
- Schemas Pydantic para valida√ß√£o

**LangGraph v1.0+** fornece a orquestra√ß√£o:
- `StateGraph` - Grafos de estado para fluxos complexos
- Checkpointers - Persist√™ncia e recupera√ß√£o de estado
- Streaming - Respostas em tempo real
- Human-in-the-Loop - Interven√ß√£o humana

**Resumo**: Use LangChain para os **componentes** e LangGraph para **coorden√°-los**.

### 1.3 Configura√ß√£o do Ambiente

#### Pr√©-requisitos
- Python 3.10 ou superior
- Uma chave de API da OpenAI (ou outro provedor)

#### Criando o Projeto

```bash
# Criar diret√≥rio do projeto
mkdir meu_agente
cd meu_agente

# Criar ambiente virtual (recomendado)
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou: venv\Scripts\activate  # Windows

# Instalar depend√™ncias com pip
pip install langchain>=1.0.0 langchain-openai>=0.3.0 langgraph>=1.0.0 python-dotenv
```

**Alternativa com uv (mais r√°pido)**:
```bash
# Instalar uv usando pip (se ainda n√£o tiver)
pip install uv
# Instalar uv standalone com curl
curl -LsSf https://astral.sh/uv/install.sh | sh
# Instalar uv standalone com wget
wget -qO- https://astral.sh/uv/install.sh | sh
# Criar projeto e instalar depend√™ncias
uv init meu_agente
cd meu_agente
uv add langchain langchain-openai langgraph python-dotenv
```

#### Configurando Vari√°veis de Ambiente

Crie um arquivo `.env` na raiz do projeto:

```env
# .env
OPENAI_API_KEY=sk-sua-chave-aqui
OPENAI_MODEL=gpt-4o-mini
```

> **Importante**: Nunca commite o arquivo `.env` no Git! Adicione-o ao `.gitignore`.

### 1.4 Primeira Chamada a um LLM

Vamos criar nosso primeiro programa que se comunica com um LLM.

```python
# hello_llm.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage

# Carregar vari√°veis de ambiente do arquivo .env
load_dotenv()

# Criar inst√¢ncia do modelo
modelo = ChatOpenAI(
    model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
    temperature=0  # 0 = determin√≠stico, 1 = criativo
)

# Criar uma mensagem do usu√°rio
mensagem = HumanMessage(content="Ol√°! Qual √© a capital do Brasil?")

# Invocar o modelo
resposta = modelo.invoke([mensagem])

# Exibir a resposta
print(f"Resposta do modelo: {resposta.content}")
```

**Executando**:
```bash
python hello_llm.py
# Sa√≠da: Resposta do modelo: A capital do Brasil √© Bras√≠lia.
```

### 1.5 Entendendo o C√≥digo

Vamos analisar cada parte:

#### 1. Importa√ß√µes
```python
from langchain_openai import ChatOpenAI          # Interface para OpenAI
from langchain_core.messages import HumanMessage # Tipo de mensagem
```

O pacote `langchain_openai` √© separado do `langchain` principal. Isso segue o padr√£o modular do LangChain v1.0+, onde cada provedor tem seu pr√≥prio pacote.

#### 2. Cria√ß√£o do Modelo
```python
modelo = ChatOpenAI(
    model="gpt-4o-mini",  # Modelo a usar
    temperature=0          # Controle de aleatoriedade
)
```

Par√¢metros importantes:
- `model`: Qual modelo usar (gpt-4o, gpt-4o-mini, gpt-3.5-turbo)
- `temperature`: 0.0 (determin√≠stico) a 1.0 (criativo)
- `max_tokens`: Limite de tokens na resposta
- `timeout`: Tempo m√°ximo de espera

#### 3. Mensagens
```python
mensagem = HumanMessage(content="Ol√°!")
resposta = modelo.invoke([mensagem])
```

O modelo recebe uma **lista de mensagens**. Isso √© fundamental para manter conversas, como veremos no pr√≥ximo cap√≠tulo.

### 1.6 Exemplo Completo: Assistente Simples

Vamos criar um assistente que responde perguntas em um loop:

```python
# assistente_simples.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage

load_dotenv()

def criar_assistente():
    """Cria e retorna uma inst√¢ncia do modelo."""
    return ChatOpenAI(
        model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
        temperature=0.7
    )

def conversar(modelo, pergunta: str) -> str:
    """Envia uma pergunta ao modelo e retorna a resposta."""
    mensagens = [
        SystemMessage(content="Voc√™ √© um assistente prestativo que responde em portugu√™s."),
        HumanMessage(content=pergunta)
    ]
    resposta = modelo.invoke(mensagens)
    return resposta.content

def main():
    print("=== Assistente Simples ===")
    print("Digite 'sair' para encerrar.\n")

    modelo = criar_assistente()

    while True:
        pergunta = input("Voc√™: ").strip()

        if pergunta.lower() == 'sair':
            print("At√© logo!")
            break

        if not pergunta:
            continue

        resposta = conversar(modelo, pergunta)
        print(f"Assistente: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Executando**:
```bash
python assistente_simples.py
```

```
=== Assistente Simples ===
Digite 'sair' para encerrar.

Voc√™: Qual √© o maior planeta do sistema solar?
Assistente: O maior planeta do sistema solar √© J√∫piter.

Voc√™: E qual √© o menor?
Assistente: O menor planeta do sistema solar √© Merc√∫rio.

Voc√™: sair
At√© logo!
```

> **Nota**: Este assistente ainda n√£o tem mem√≥ria - cada pergunta √© independente. No pr√≥ximo cap√≠tulo, aprenderemos como manter o contexto da conversa.

### 1.7 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- **LLMs** s√£o modelos de linguagem que compreendem e geram texto
- **LangChain** fornece componentes (mensagens, modelos, tools)
- **LangGraph** fornece orquestra√ß√£o (grafos, estado, persist√™ncia)
- Como **configurar o ambiente** com Python e vari√°veis de ambiente
- Como fazer sua **primeira chamada** a um LLM
- A estrutura b√°sica de **mensagens** (HumanMessage, SystemMessage)

### 1.8 Exerc√≠cios

1. **Modifique o assistente** para usar `temperature=0` e depois `temperature=1`. Observe as diferen√ßas nas respostas.

2. **Crie um tradutor** que receba texto em portugu√™s e traduza para ingl√™s.

3. **Experimente diferentes modelos**: Troque `gpt-4o-mini` por `gpt-4o` e compare a qualidade das respostas.

---

## Cap√≠tulo 2: Mensagens e Prompts

### 2.1 Tipos de Mensagens no LangChain

No LangChain v1.0+, a comunica√ß√£o com LLMs √© baseada em **mensagens tipadas**. Cada tipo de mensagem tem um papel espec√≠fico na conversa:

```python
from langchain_core.messages import (
    SystemMessage,   # Instru√ß√µes do sistema
    HumanMessage,    # Mensagens do usu√°rio
    AIMessage,       # Respostas do assistente
    ToolMessage,     # Resultados de ferramentas
)
```

#### SystemMessage - Definindo o Comportamento

A `SystemMessage` define **quem o assistente √©** e **como ele deve se comportar**. √â sempre a primeira mensagem da conversa.

```python
from langchain_core.messages import SystemMessage

system = SystemMessage(content="""
Voc√™ √© um assistente especializado em programa√ß√£o Python.
Responda sempre em portugu√™s brasileiro.
Seja conciso e forne√ßa exemplos de c√≥digo quando apropriado.
""")
```

#### HumanMessage - Entrada do Usu√°rio

A `HumanMessage` representa as mensagens enviadas pelo usu√°rio:

```python
from langchain_core.messages import HumanMessage

pergunta = HumanMessage(content="Como criar uma lista em Python?")
```

#### AIMessage - Resposta do Assistente

A `AIMessage` representa as respostas geradas pelo modelo. Voc√™ a recebe como retorno do `invoke()`:

```python
resposta = modelo.invoke([system, pergunta])
# resposta √© uma AIMessage
print(type(resposta))  # <class 'langchain_core.messages.ai.AIMessage'>
print(resposta.content)  # "Para criar uma lista em Python..."
```

#### ToolMessage - Resultado de Ferramentas

A `ToolMessage` carrega o resultado da execu√ß√£o de uma ferramenta. Veremos isso em detalhes no Cap√≠tulo 3.

```python
from langchain_core.messages import ToolMessage

resultado = ToolMessage(
    content="A tarefa foi criada com sucesso.",
    tool_call_id="call_abc123"  # ID da chamada da ferramenta
)
```

### 2.2 Estrutura de uma Conversa

Uma conversa √© uma **lista de mensagens** que cresce ao longo do tempo:

```python
# conversa_estruturada.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

load_dotenv()

modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

# Hist√≥rico da conversa
historico = [
    SystemMessage(content="Voc√™ √© um professor de hist√≥ria. Responda em portugu√™s."),
    HumanMessage(content="Quem descobriu o Brasil?"),
    AIMessage(content="O Brasil foi oficialmente descoberto por Pedro √Ålvares Cabral em 22 de abril de 1500."),
    HumanMessage(content="E em que cidade ele desembarcou?"),
]

# O modelo tem acesso a todo o hist√≥rico
resposta = modelo.invoke(historico)
print(resposta.content)
# Sa√≠da: Pedro √Ålvares Cabral desembarcou na regi√£o que hoje √© Porto Seguro, na Bahia.
```

> **Importante**: O modelo n√£o tem mem√≥ria interna. Voc√™ precisa enviar **todo o hist√≥rico** a cada chamada para manter o contexto.

### 2.3 Chatbot com Mem√≥ria Manual

Vamos criar um chatbot que mant√©m o hist√≥rico da conversa:

```python
# chatbot_com_memoria.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

class Chatbot:
    def __init__(self, instrucoes: str):
        self.modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7
        )
        self.historico = [SystemMessage(content=instrucoes)]

    def conversar(self, mensagem: str) -> str:
        # Adicionar mensagem do usu√°rio ao hist√≥rico
        self.historico.append(HumanMessage(content=mensagem))

        # Obter resposta do modelo
        resposta = self.modelo.invoke(self.historico)

        # Adicionar resposta ao hist√≥rico
        self.historico.append(resposta)

        return resposta.content

    def limpar_historico(self):
        # Mant√©m apenas a SystemMessage
        self.historico = [self.historico[0]]

def main():
    bot = Chatbot(
        instrucoes="Voc√™ √© um assistente amig√°vel. Responda em portugu√™s de forma concisa."
    )

    print("=== Chatbot com Mem√≥ria ===")
    print("Comandos: 'sair' para encerrar, 'limpar' para reiniciar conversa\n")

    while True:
        entrada = input("Voc√™: ").strip()

        if entrada.lower() == 'sair':
            print("At√© logo!")
            break
        elif entrada.lower() == 'limpar':
            bot.limpar_historico()
            print("Hist√≥rico limpo!\n")
            continue
        elif not entrada:
            continue

        resposta = bot.conversar(entrada)
        print(f"Bot: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Testando a mem√≥ria**:
```
Voc√™: Meu nome √© Jo√£o
Bot: Ol√°, Jo√£o! Prazer em conhec√™-lo. Como posso ajud√°-lo hoje?

Voc√™: Qual √© o meu nome?
Bot: Seu nome √© Jo√£o, conforme voc√™ me disse agora h√° pouco.
```

### 2.4 Templates de Prompt Din√¢micos

Muitas vezes precisamos criar prompts com vari√°veis din√¢micas. O LangChain oferece templates para isso:

```python
# prompt_dinamico.py
from datetime import datetime
from langchain_core.prompts import ChatPromptTemplate

# Template com vari√°veis
template = ChatPromptTemplate.from_messages([
    ("system", """Voc√™ √© um assistente pessoal.
Data atual: {data_atual}
Nome do usu√°rio: {nome_usuario}
Responda sempre de forma personalizada."""),
    ("human", "{pergunta}")
])

# Preencher as vari√°veis
mensagens = template.invoke({
    "data_atual": datetime.now().strftime("%d/%m/%Y"),
    "nome_usuario": "Maria",
    "pergunta": "Que dia √© hoje?"
})

print(mensagens)
# Sa√≠da: lista de mensagens com as vari√°veis substitu√≠das
```

### 2.5 Inje√ß√£o de Contexto: Data e Hora Atual

Um padr√£o comum √© injetar informa√ß√µes din√¢micas no system prompt. Veja como o projeto JarvisChat faz isso:

```python
# prompt_com_data.py
import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

def get_system_prompt() -> str:
    """
    Gera o system prompt com data/hora atual.
    IMPORTANTE: Chamar a cada invoca√ß√£o para garantir data atualizada.
    """
    agora = datetime.now()

    # Tradu√ß√£o dos dias da semana
    dias_semana = {
        'Monday': 'segunda-feira',
        'Tuesday': 'ter√ßa-feira',
        'Wednesday': 'quarta-feira',
        'Thursday': 'quinta-feira',
        'Friday': 'sexta-feira',
        'Saturday': 's√°bado',
        'Sunday': 'domingo'
    }

    dia_semana = dias_semana.get(agora.strftime('%A'), agora.strftime('%A'))
    data_formatada = agora.strftime('%d/%m/%Y')
    hora_formatada = agora.strftime('%H:%M')

    return f"""Voc√™ √© um assistente pessoal inteligente.

## Informa√ß√µes Temporais
- Data atual: {data_formatada} ({dia_semana})
- Hora atual: {hora_formatada}

## Instru√ß√µes
- Responda sempre em portugu√™s brasileiro
- Use as informa√ß√µes temporais quando relevante
- Seja cordial e prestativo
"""

def main():
    modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))

    mensagens = [
        SystemMessage(content=get_system_prompt()),
        HumanMessage(content="Que dia √© hoje? E que horas s√£o?")
    ]

    resposta = modelo.invoke(mensagens)
    print(resposta.content)

if __name__ == "__main__":
    main()
```

**Sa√≠da exemplo**:
```
Hoje √© 10/12/2025, uma ter√ßa-feira, e s√£o aproximadamente 15:30.
```

### 2.6 Boas Pr√°ticas para System Prompts

Um bom system prompt deve ser:

1. **Claro e espec√≠fico**: Defina exatamente o papel do assistente
2. **Estruturado**: Use se√ß√µes e formata√ß√£o
3. **Contextualizado**: Inclua informa√ß√µes relevantes (data, usu√°rio, etc.)
4. **Com exemplos**: Mostre o formato esperado das respostas

**Exemplo de system prompt bem estruturado**:

```python
SYSTEM_PROMPT = """
# Papel
Voc√™ √© um assistente de gerenciamento de tarefas.

# Capacidades
- Criar, listar, atualizar e excluir tarefas
- Organizar tarefas por categorias
- Definir datas de vencimento

# Regras
1. Sempre confirme a√ß√µes destrutivas (exclus√£o)
2. Use formato de data brasileiro (DD/MM/AAAA)
3. Seja conciso nas respostas

# Formato de Resposta
- Para listagem: use bullets (-)
- Para confirma√ß√µes: use ‚úì ou ‚úó
- Para datas: sempre em portugu√™s

# Contexto
Data atual: {data_atual}
Usu√°rio: {nome_usuario}
"""
```

### 2.7 Exemplo Completo: Assistente com Contexto

```python
# assistente_contextualizado.py
import os
from datetime import datetime
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

load_dotenv()

class AssistenteContextualizado:
    def __init__(self, nome_usuario: str):
        self.modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0.7
        )
        self.nome_usuario = nome_usuario
        self.historico = []

    def _get_system_prompt(self) -> str:
        agora = datetime.now()
        return f"""Voc√™ √© um assistente pessoal chamado Jarvis.

## Contexto
- Usu√°rio: {self.nome_usuario}
- Data: {agora.strftime('%d/%m/%Y')}
- Hora: {agora.strftime('%H:%M')}

## Personalidade
- Seja cordial e use o nome do usu√°rio ocasionalmente
- Responda em portugu√™s brasileiro
- Seja conciso, mas completo
"""

    def conversar(self, mensagem: str) -> str:
        # System prompt atualizado a cada chamada (data/hora atual)
        system = SystemMessage(content=self._get_system_prompt())

        # Montar mensagens: system + hist√≥rico + nova mensagem
        mensagens = [system] + self.historico + [HumanMessage(content=mensagem)]

        # Obter resposta
        resposta = self.modelo.invoke(mensagens)

        # Atualizar hist√≥rico (sem o system, que √© recriado)
        self.historico.append(HumanMessage(content=mensagem))
        self.historico.append(resposta)

        return resposta.content

def main():
    nome = input("Qual √© o seu nome? ").strip() or "Usu√°rio"
    assistente = AssistenteContextualizado(nome_usuario=nome)

    print(f"\nOl√°, {nome}! Sou o Jarvis, seu assistente pessoal.")
    print("Digite 'sair' para encerrar.\n")

    while True:
        entrada = input("Voc√™: ").strip()
        if entrada.lower() == 'sair':
            print(f"At√© logo, {nome}!")
            break
        if entrada:
            resposta = assistente.conversar(entrada)
            print(f"Jarvis: {resposta}\n")

if __name__ == "__main__":
    main()
```

### 2.8 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- Os **4 tipos de mensagens**: SystemMessage, HumanMessage, AIMessage, ToolMessage
- Como **estruturar conversas** com listas de mensagens
- A implementar **mem√≥ria manual** mantendo o hist√≥rico
- A criar **templates din√¢micos** com vari√°veis
- Como **injetar contexto** (data, hora, usu√°rio) no prompt
- **Boas pr√°ticas** para system prompts eficazes

### 2.9 Exerc√≠cios

1. **Crie um chatbot tem√°tico**: Um assistente especializado em receitas culin√°rias que pergunta sobre ingredientes dispon√≠veis.

2. **Limite de contexto**: Modifique o chatbot para manter apenas as √∫ltimas 10 mensagens no hist√≥rico (evitando estourar o limite de tokens).

3. **Prompt multil√≠ngue**: Crie um assistente que detecta o idioma da pergunta e responde no mesmo idioma.

---

## Cap√≠tulo 3: Tools (Ferramentas)

### 3.1 O Que S√£o Tools e Por Que S√£o Importantes?

**Tools** (ferramentas) s√£o fun√ß√µes que o modelo pode **decidir chamar** para realizar a√ß√µes no mundo real. Sem tools, o modelo √© apenas um gerador de texto. Com tools, ele se torna um **agente** capaz de:

- Buscar informa√ß√µes em bancos de dados
- Fazer c√°lculos matem√°ticos
- Criar, atualizar e excluir dados
- Interagir com APIs externas
- Executar c√≥digo

#### Fluxo de Execu√ß√£o com Tools

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Usu√°rio   ‚îÇ ‚îÄ‚îÄ‚ñ∂  ‚îÇ    LLM      ‚îÇ ‚îÄ‚îÄ‚ñ∂  ‚îÇ    Tool     ‚îÇ
‚îÇ  "Calcule   ‚îÇ      ‚îÇ  (decide    ‚îÇ      ‚îÇ (executa    ‚îÇ
‚îÇ   2 + 2"    ‚îÇ      ‚îÇ  usar tool) ‚îÇ      ‚îÇ  c√°lculo)   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                    ‚îÇ
                            ‚ñº                    ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ    LLM      ‚îÇ ‚óÄ‚îÄ‚îÄ  ‚îÇ  Resultado  ‚îÇ
                     ‚îÇ  (formata   ‚îÇ      ‚îÇ    "4"      ‚îÇ
                     ‚îÇ  resposta)  ‚îÇ      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ
                            ‚ñº
                     ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                     ‚îÇ  "2 + 2 = 4"‚îÇ
                     ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

### 3.2 Criando Tools com o Decorator @tool

O LangChain v1.0+ usa o decorator `@tool` para criar ferramentas:

```python
# tool_simples.py
from langchain_core.tools import tool

@tool
def somar(a: int, b: int) -> int:
    """Soma dois n√∫meros inteiros.

    Args:
        a: Primeiro n√∫mero
        b: Segundo n√∫mero

    Returns:
        A soma dos dois n√∫meros
    """
    return a + b

# Inspecionar a tool
print(f"Nome: {somar.name}")
print(f"Descri√ß√£o: {somar.description}")
print(f"Schema: {somar.args_schema.schema()}")
```

**Sa√≠da**:
```
Nome: somar
Descri√ß√£o: Soma dois n√∫meros inteiros.
Schema: {'properties': {'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}, 'required': ['a', 'b'], 'type': 'object'}
```

> **Importante**: A **docstring** √© fundamental! O LLM usa a descri√ß√£o para decidir quando usar a tool.

### 3.3 Tools com Schemas Pydantic

Para tools mais complexas, use modelos Pydantic para valida√ß√£o:

```python
# tool_com_pydantic.py
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from typing import Optional
from datetime import date

class CriarTarefaInput(BaseModel):
    """Schema para cria√ß√£o de tarefas."""
    titulo: str = Field(description="T√≠tulo da tarefa")
    descricao: Optional[str] = Field(default=None, description="Descri√ß√£o detalhada")
    data_vencimento: Optional[date] = Field(default=None, description="Data de vencimento (YYYY-MM-DD)")

@tool(args_schema=CriarTarefaInput)
def criar_tarefa(titulo: str, descricao: Optional[str] = None, data_vencimento: Optional[date] = None) -> str:
    """Cria uma nova tarefa no sistema.

    Use esta ferramenta quando o usu√°rio quiser adicionar uma nova tarefa,
    atividade ou lembrete.
    """
    # Simula√ß√£o - em produ√ß√£o, salvaria no banco de dados
    tarefa_id = 123
    resultado = f"Tarefa criada com sucesso!\n"
    resultado += f"- ID: {tarefa_id}\n"
    resultado += f"- T√≠tulo: {titulo}\n"
    if descricao:
        resultado += f"- Descri√ß√£o: {descricao}\n"
    if data_vencimento:
        resultado += f"- Vencimento: {data_vencimento.strftime('%d/%m/%Y')}\n"
    return resultado

# Testar a tool diretamente
print(criar_tarefa.invoke({
    "titulo": "Estudar LangChain",
    "descricao": "Completar tutorial",
    "data_vencimento": "2025-12-15"
}))
```

### 3.4 Binding Tools ao Modelo

Para que o modelo possa usar as tools, precisamos "bind√°-las":

```python
# binding_tools.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage

load_dotenv()

# Definir tools
@tool
def calcular(expressao: str) -> str:
    """Calcula uma express√£o matem√°tica simples.

    Args:
        expressao: Express√£o matem√°tica (ex: "2 + 2", "10 * 5")
    """
    try:
        # ATEN√á√ÉO: eval() √© perigoso em produ√ß√£o!
        # Use uma biblioteca segura como numexpr
        resultado = eval(expressao)
        return f"Resultado: {resultado}"
    except Exception as e:
        return f"Erro no c√°lculo: {e}"

@tool
def obter_clima(cidade: str) -> str:
    """Obt√©m informa√ß√µes sobre o clima de uma cidade.

    Args:
        cidade: Nome da cidade
    """
    # Simula√ß√£o - em produ√ß√£o, chamaria uma API real
    climas = {
        "s√£o paulo": "Nublado, 22¬∞C",
        "rio de janeiro": "Ensolarado, 32¬∞C",
        "curitiba": "Chuvoso, 15¬∞C",
    }
    return climas.get(cidade.lower(), f"Clima n√£o dispon√≠vel para {cidade}")

# Criar modelo COM tools bindadas
modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
modelo_com_tools = modelo.bind_tools([calcular, obter_clima])

# Testar - o modelo decide qual tool usar
resposta = modelo_com_tools.invoke([
    HumanMessage(content="Quanto √© 15 vezes 8?")
])

print(f"Conte√∫do: {resposta.content}")
print(f"Tool calls: {resposta.tool_calls}")
```

**Sa√≠da**:
```
Conte√∫do:
Tool calls: [{'name': 'calcular', 'args': {'expressao': '15 * 8'}, 'id': 'call_abc123', 'type': 'tool_call'}]
```

> **Observe**: Quando o modelo decide usar uma tool, o `content` fica vazio e os argumentos v√£o em `tool_calls`.

### 3.5 Executando Tools e Retornando Resultados

O fluxo completo envolve:
1. Enviar mensagem ao modelo
2. Verificar se h√° tool_calls
3. Executar as tools
4. Enviar resultados de volta ao modelo
5. Obter resposta final

```python
# executar_tools.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import HumanMessage, ToolMessage

load_dotenv()

@tool
def calcular(expressao: str) -> str:
    """Calcula uma express√£o matem√°tica."""
    try:
        resultado = eval(expressao)
        return str(resultado)
    except Exception as e:
        return f"Erro: {e}"

@tool
def obter_clima(cidade: str) -> str:
    """Obt√©m o clima de uma cidade."""
    climas = {"s√£o paulo": "22¬∞C, Nublado", "rio de janeiro": "32¬∞C, Sol"}
    return climas.get(cidade.lower(), "Dados n√£o dispon√≠veis")

# Mapear tools por nome
tools = [calcular, obter_clima]
tools_por_nome = {t.name: t for t in tools}

# Modelo com tools
modelo = ChatOpenAI(model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"))
modelo_com_tools = modelo.bind_tools(tools)

def processar_com_tools(mensagem: str) -> str:
    """Processa uma mensagem, executando tools se necess√°rio."""
    mensagens = [HumanMessage(content=mensagem)]

    # Primeira chamada ao modelo
    resposta = modelo_com_tools.invoke(mensagens)
    mensagens.append(resposta)

    # Se houver tool_calls, executar
    while resposta.tool_calls:
        for tool_call in resposta.tool_calls:
            nome_tool = tool_call["name"]
            args = tool_call["args"]
            tool_call_id = tool_call["id"]

            print(f"Executando tool: {nome_tool}({args})")

            # Executar a tool
            tool_fn = tools_por_nome[nome_tool]
            resultado = tool_fn.invoke(args)

            # Adicionar resultado como ToolMessage
            mensagens.append(ToolMessage(
                content=resultado,
                tool_call_id=tool_call_id
            ))

        # Nova chamada ao modelo com os resultados
        resposta = modelo_com_tools.invoke(mensagens)
        mensagens.append(resposta)

    return resposta.content

# Testar
print(processar_com_tools("Quanto √© 25 ao quadrado?"))
print("\n---\n")
print(processar_com_tools("Como est√° o clima em S√£o Paulo?"))
```

**Sa√≠da**:
```
Executando tool: calcular({'expressao': '25 ** 2'})
25 ao quadrado √© igual a 625.

---

Executando tool: obter_clima({'cidade': 'S√£o Paulo'})
O clima em S√£o Paulo est√° em torno de 22¬∞C, com c√©u nublado.
```

### 3.6 Tool com M√∫ltiplos Par√¢metros

Vamos criar uma tool mais complexa para gerenciar tarefas:

```python
# tool_tarefas.py
from langchain_core.tools import tool
from pydantic import BaseModel, Field
from typing import Optional, Literal
from datetime import date
from enum import Enum

class EstadoTarefa(str, Enum):
    PENDENTE = "pendente"
    CONCLUIDA = "concluida"
    ARQUIVADA = "arquivada"

class ListarTarefasInput(BaseModel):
    """Schema para listagem de tarefas."""
    estado: Optional[Literal["pendente", "concluida", "arquivada"]] = Field(
        default=None,
        description="Filtrar por estado da tarefa"
    )
    categoria: Optional[str] = Field(
        default=None,
        description="Filtrar por categoria"
    )

# Banco de dados simulado
TAREFAS_DB = [
    {"id": 1, "titulo": "Estudar Python", "estado": "pendente", "categoria": "Estudos"},
    {"id": 2, "titulo": "Fazer compras", "estado": "concluida", "categoria": "Pessoal"},
    {"id": 3, "titulo": "Reuni√£o de equipe", "estado": "pendente", "categoria": "Trabalho"},
]

@tool(args_schema=ListarTarefasInput)
def listar_tarefas(
    estado: Optional[str] = None,
    categoria: Optional[str] = None
) -> str:
    """Lista as tarefas do usu√°rio com filtros opcionais.

    Use esta ferramenta para mostrar tarefas existentes.
    Pode filtrar por estado (pendente, concluida, arquivada) e/ou categoria.
    """
    tarefas = TAREFAS_DB.copy()

    # Aplicar filtros
    if estado:
        tarefas = [t for t in tarefas if t["estado"] == estado]
    if categoria:
        tarefas = [t for t in tarefas if t["categoria"].lower() == categoria.lower()]

    if not tarefas:
        return "Nenhuma tarefa encontrada com os filtros especificados."

    # Formatar resultado
    resultado = f"Encontradas {len(tarefas)} tarefa(s):\n\n"
    for t in tarefas:
        emoji = "‚è≥" if t["estado"] == "pendente" else "‚úÖ" if t["estado"] == "concluida" else "üì¶"
        resultado += f"{emoji} [{t['id']}] {t['titulo']}\n"
        resultado += f"   Categoria: {t['categoria']} | Estado: {t['estado']}\n\n"

    return resultado

# Testar
print(listar_tarefas.invoke({"estado": "pendente"}))
```

### 3.7 Boas Pr√°ticas para Tools

1. **Docstrings descritivas**: O LLM usa a descri√ß√£o para decidir quando usar a tool
2. **Nomes claros**: Use verbos no infinitivo (criar, listar, atualizar, excluir)
3. **Valida√ß√£o com Pydantic**: Garante que os argumentos est√£o corretos
4. **Tratamento de erros**: Retorne mensagens de erro √∫teis, n√£o exce√ß√µes
5. **Retorno informativo**: Confirme o que foi feito, n√£o apenas "sucesso"

```python
# Exemplo de tool bem documentada
@tool
def criar_tarefa(titulo: str, data_vencimento: Optional[str] = None) -> str:
    """Cria uma nova tarefa no sistema de gerenciamento.

    Use esta ferramenta quando o usu√°rio quiser:
    - Adicionar uma nova tarefa
    - Criar um lembrete
    - Agendar uma atividade

    Args:
        titulo: T√≠tulo descritivo da tarefa (obrigat√≥rio)
        data_vencimento: Data limite no formato YYYY-MM-DD (opcional)

    Returns:
        Confirma√ß√£o com detalhes da tarefa criada

    Exemplos de uso:
        - "Crie uma tarefa para estudar Python"
        - "Adicione lembrete: reuni√£o dia 15/12"
    """
    # Implementa√ß√£o...
```

### 3.8 Exemplo Completo: Assistente com Tools

```python
# assistente_com_tools.py
import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI
from langchain_core.tools import tool
from langchain_core.messages import SystemMessage, HumanMessage, ToolMessage

load_dotenv()

# === TOOLS ===

@tool
def somar(a: float, b: float) -> str:
    """Soma dois n√∫meros."""
    return str(a + b)

@tool
def subtrair(a: float, b: float) -> str:
    """Subtrai b de a."""
    return str(a - b)

@tool
def multiplicar(a: float, b: float) -> str:
    """Multiplica dois n√∫meros."""
    return str(a * b)

@tool
def dividir(a: float, b: float) -> str:
    """Divide a por b."""
    if b == 0:
        return "Erro: divis√£o por zero n√£o √© permitida"
    return str(a / b)

# === ASSISTENTE ===

class AssistenteCalculadora:
    def __init__(self):
        self.tools = [somar, subtrair, multiplicar, dividir]
        self.tools_por_nome = {t.name: t for t in self.tools}

        modelo = ChatOpenAI(
            model=os.getenv("OPENAI_MODEL", "gpt-4o-mini"),
            temperature=0
        )
        self.modelo = modelo.bind_tools(self.tools)

        self.system = SystemMessage(content="""
Voc√™ √© uma calculadora inteligente.
Use as ferramentas dispon√≠veis para fazer c√°lculos.
Sempre mostre o resultado de forma clara.
""")

    def processar(self, pergunta: str) -> str:
        mensagens = [self.system, HumanMessage(content=pergunta)]

        while True:
            resposta = self.modelo.invoke(mensagens)
            mensagens.append(resposta)

            # Se n√£o h√° tool_calls, retornar resposta final
            if not resposta.tool_calls:
                return resposta.content

            # Executar cada tool
            for tool_call in resposta.tool_calls:
                nome = tool_call["name"]
                args = tool_call["args"]

                resultado = self.tools_por_nome[nome].invoke(args)

                mensagens.append(ToolMessage(
                    content=resultado,
                    tool_call_id=tool_call["id"]
                ))

def main():
    calc = AssistenteCalculadora()

    print("=== Calculadora Inteligente ===")
    print("Digite 'sair' para encerrar.\n")

    while True:
        entrada = input("Voc√™: ").strip()
        if entrada.lower() == 'sair':
            break
        if entrada:
            resposta = calc.processar(entrada)
            print(f"Calculadora: {resposta}\n")

if __name__ == "__main__":
    main()
```

**Testando**:
```
Voc√™: Quanto √© 15 vezes 8?
Calculadora: 15 vezes 8 √© igual a 120.

Voc√™: Agora divida o resultado por 3
Calculadora: 120 dividido por 3 √© igual a 40.

Voc√™: Some 100 e 200, depois multiplique por 2
Calculadora: (100 + 200) √ó 2 = 600
```

### 3.9 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- O que s√£o **tools** e por que s√£o essenciais para agentes
- Como criar tools com o **decorator @tool**
- Como usar **Pydantic** para valida√ß√£o de argumentos
- Como fazer **bind_tools()** ao modelo
- O fluxo completo de **execu√ß√£o de tools**
- **Boas pr√°ticas** para documenta√ß√£o de tools

### 3.10 Exerc√≠cios

1. **Crie uma tool de convers√£o**: Converta temperaturas entre Celsius e Fahrenheit.

2. **Tool de busca**: Crie uma tool que simula busca em uma lista de produtos.

3. **M√∫ltiplas tools**: Crie um assistente com tools para CRUD completo de uma entidade.

---

# PARTE II: LCEL E COMPOSI√á√ÉO DECLARATIVA

---

## Cap√≠tulo 4: LangChain Expression Language (LCEL)

### 4.1 O Que √â LCEL?

At√© agora, constru√≠mos pipelines "manualmente": criamos prompts, passamos para modelos, parseamos outputs. √Ä medida que sistemas crescem, esse padr√£o se torna repetitivo e fr√°gil.

**LangChain Expression Language (LCEL)** √© a resposta: uma linguagem declarativa para compor componentes LangChain usando o operador **pipe** (`|`).

> **LCEL** permite construir pipelines reutiliz√°veis, test√°veis e serializ√°veis sem "glue code" manual.

### 4.2 Composi√ß√£o com Pipes

A ideia √© simples: componentes podem ser encadeados com `|`, criando uma pipeline declarativa:

```python
# Antes: manual
prompt = ChatPromptTemplate.from_messages([...])
model = ChatOpenAI(...)
parser = StrOutputParser()

messages = prompt.invoke({"topic": "Python"})
response = model.invoke(messages)
resposta_final = parser.invoke(response)

# Depois: LCEL com pipes
chain = prompt | model | parser
resposta_final = chain.invoke({"topic": "Python"})
```

O operador `|` √© sintaticamente a√ß√∫car, mas semanticamente poderoso: transforma fun√ß√µes em objetos compostos que sabem como:
- Invocar (`.invoke()`)
- Fazer streaming (`.stream()`)
- Executar em batch (`.batch()`)

### 4.3 Conceitos Essenciais de RAG (Retrieval Augmented Generation)

Antes de explorar exemplos pr√°ticos de LCEL, precisamos entender alguns conceitos fundamentais que aparecem nos pr√≥ximos exemplos.

#### 4.3.1 O que √© RAG?

**RAG (Retrieval Augmented Generation)** √© uma arquitetura que combina dois componentes:
1. **Retrieval** (Busca) - encontrar documentos relevantes
2. **Generation** (Gera√ß√£o) - usar um LLM para criar respostas baseadas nos documentos

**Por que RAG?**
- LLMs treinados t√™m conhecimento limitado e podem ficar desatualizados
- RAG permite adicionar conhecimento externo (seus documentos, bases de dados, etc.)
- Respostas s√£o baseadas em fontes confi√°veis e atualizadas
- Reduz alucina√ß√µes (respostas inventadas) do modelo

**Fluxo b√°sico de uma arquitetura RAG**:
```
Pergunta do Usu√°rio
    ‚Üì
Buscar Documentos Relevantes (Retriever)
    ‚Üì
Passar Documentos + Pergunta para o LLM
    ‚Üì
LLM Gera Resposta com Base no Contexto
    ‚Üì
Resposta Fundamentada
```

#### 4.3.2 Embeddings - Representa√ß√µes Vetoriais

**O que s√£o?**
- Vetores num√©ricos que representam o significado de um texto
- Cada palavra ou documento √© convertido em uma lista de n√∫meros
- Textos com significado similar t√™m vetores pr√≥ximos no espa√ßo vetorial

**Exemplo Visual**:
```
Texto: "cachorro"      ‚Üí Vetor: [0.2, 0.8, 0.1, 0.5, ...]
Texto: "c√£o"           ‚Üí Vetor: [0.19, 0.82, 0.09, 0.51, ...]  ‚Üê Muito Similar!
Texto: "planeta Terra" ‚Üí Vetor: [0.91, 0.05, 0.87, 0.2, ...]   ‚Üê Diferente
```

**Como funcionam?**
- Modelos treinados (OpenAI, Cohere, HuggingFace, etc.) convertam texto em vetores
- Esses vetores codificam informa√ß√µes sem√¢nticas
- A similaridade entre vetores pode ser medida (ex: produto escalar, cosseno)

**Por que √© importante?**
- Permite buscar documentos por significado, n√£o por palavras-chave
- Base do funcionamento de vector stores

#### 4.3.3 Vector Stores - Bancos de Dados de Vetores

**O que s√£o?**
- Bancos de dados especializados em armazenar e buscar vetores
- Otimizados para encontrar vetores similares rapidamente
- Exemplos populares: FAISS, Chroma, Pinecone, Qdrant, Weaviate

**Opera√ß√µes principais**:
1. **Adicionar documentos**:
   - Documento original ‚Üí Embedding (vetor) ‚Üí Armazenar no banco

2. **Buscar documentos**:
   - Query do usu√°rio ‚Üí Embedding (vetor) ‚Üí Buscar K vetores mais similares ‚Üí Retornar documentos originais

**Exemplo Visual**:
```
Vector Store
‚îú‚îÄ "Manual Python" ‚Üí [0.1, 0.9, 0.2, ...]
‚îú‚îÄ "Guia JavaScript" ‚Üí [0.2, 0.8, 0.3, ...]
‚îú‚îÄ "Tutorial LangChain" ‚Üí [0.15, 0.85, 0.25, ...]
‚îî‚îÄ "Receita de Bolo" ‚Üí [0.8, 0.1, 0.9, ...]

Busca: "Como usar Python?" ‚Üí Embedding ‚Üí [0.11, 0.88, 0.21, ...]
Resultado: "Manual Python" (mais similar), "Tutorial LangChain" (2¬∫ mais similar)
```

#### 4.3.4 Retriever - Interface de Busca Simplificada

**O que √©?**
- Uma abstra√ß√£o que encapsula a busca em vector stores
- Interface padr√£o para recuperar documentos relevantes
- Criado a partir de um vector store: `vectorstore.as_retriever()`

**Par√¢metros comuns**:
- `k`: N√∫mero de documentos a retornar (exemplo: `k=2` retorna top 2 mais similares)
- `search_type`: Tipo de busca (default: "similarity")

**Como usar**:
```python
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})
docs = retriever.invoke("minha pergunta")  # Retorna lista com 2 documentos
```

> **üìö Nota Importante**: Este √© um resumo introdut√≥rio de RAG. Os Cap√≠tulos 12, 13 e 14 exploram RAG em profundidade, incluindo t√©cnicas avan√ßadas como chunking, embedding models diferentes, prompt optimization, e retrieval strategies.

### 4.4 Runnables em LCEL - Blocos de Constru√ß√£o

LCEL usa o conceito de **Runnable** como bloco fundamental. Entender Runnables √© essencial para compor pipelines.

#### 4.4.1 O que √© um Runnable?

**Defini√ß√£o**:
- Interface padr√£o do LangChain para qualquer componente que pode ser executado
- Permite composi√ß√£o via operador `|`
- Suporta opera√ß√µes padr√£o: `invoke()`, `stream()`, `batch()`

**Exemplos de objetos que s√£o Runnables**:
- Prompts (`ChatPromptTemplate`)
- LLMs (`ChatOpenAI`, `Anthropic`)
- Parsers (`StrOutputParser`)
- Retrievers (criados de vector stores)
- Fun√ß√µes Python customizadas (via `RunnableLambda`)
- Dicion√°rios (automaticamente convertidos em `RunnableParallel`)

**Por que Runnables?**
- Interface consistente para todos os componentes
- Permite encadear qualquer coisa com `|`
- Automaticamente suporta async, streaming, batch

#### 4.4.2 RunnablePassthrough

**O que faz?**
- Passa dados **inalterados** atrav√©s da cadeia
- √ötil para preservar inputs originais em pipelines complexos
- N√£o faz nenhuma transforma√ß√£o

**Caso de Uso Principal - RAG**:
```python
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
)
```

**Explica√ß√£o do Fluxo**:
- `retriever` busca documentos baseado no input
- `RunnablePassthrough()` mant√©m a pergunta original intacta
- Ambos s√£o passados para o prompt como vari√°veis `{context}` e `{question}`

**Diagrama Visual**:
```
Input: "O que √© Python?"
    ‚îÇ
    ‚îú‚îÄ‚Üí retriever(input) ‚Üí ["Python √© uma linguagem..."] ‚Üí context
    ‚îÇ
    ‚îî‚îÄ‚Üí RunnablePassthrough() ‚Üí "O que √© Python?" ‚Üí question
         ‚îÇ
         ‚îú‚îÄ‚Üí prompt.invoke({"context": "...", "question": "O que √© Python?"})
         ‚îÇ
         ‚îú‚îÄ‚Üí model.invoke(prompt_formatted)
         ‚îÇ
         ‚îî‚îÄ‚Üí parser.invoke(model_output) ‚Üí resposta final
```

#### 4.4.3 RunnableLambda

**O que faz?**
- Transforma **qualquer fun√ß√£o Python** em um Runnable
- Permite integrar l√≥gica customizada em cadeias LCEL
- A fun√ß√£o fica "native" √† cadeia (suporta stream, batch, etc.)

**Exemplo de Uso**:
```python
def adicionar_timestamp(input):
    from datetime import datetime
    return f"[{datetime.now()}] {input}"

chain = (
    RunnableLambda(adicionar_timestamp)
    | prompt
    | model
)
```

**Quando usar RunnableLambda?**
- Transformar dados entre componentes
- Adicionar logs ou monitoramento
- Aplicar regras de neg√≥cio customizadas
- Integrar com APIs ou bancos de dados externos
- Fazer pr√©/p√≥s-processamento de texto

**Compara√ß√£o**:

| Sem RunnableLambda | Com RunnableLambda |
|-------------------|-------------------|
| Fun√ß√£o separada, fora da cadeia | Integrado √† cadeia |
| Precisa chamar manualmente | Executa automaticamente no fluxo |
| N√£o suporta `.stream()` autom√°tico | Suporta todas opera√ß√µes Runnable |
| C√≥digo mais verboso | C√≥digo mais limpo |

#### 4.4.4 Resumo - Padr√µes Comuns de Runnables

| Runnable | Entrada | Sa√≠da | Caso de Uso Principal |
|----------|---------|-------|----------------------|
| `RunnablePassthrough` | Qualquer input | Input inalterado | Passar dados atrav√©s da cadeia |
| `RunnableLambda` | Qualquer input | Resultado da fun√ß√£o | Transforma√ß√µes customizadas |
| Dicion√°rio `{...}` | Input simples | Dict com chaves | Agrupar m√∫ltiplas opera√ß√µes (automaticamente `RunnableParallel`) |
| `RunnableParallel` | Input simples | Dict com resultados | Executar m√∫ltiplas cadeias em paralelo |

**Dica**: A sintaxe `{"key": runnable}` √© automaticamente convertida em `RunnableParallel`, que executa m√∫ltiplos Runnables em paralelo e combina resultados.

### 4.5 Exemplo Pr√°tico: Pipeline RAG Simples

Vamos construir um pipeline RAG b√°sico com LCEL:

```python
# rag_lcel_simples.py
"""
Exemplo: Pipeline RAG com LCEL

Este exemplo demonstra como combinar:
- Retriever: Busca documentos relevantes (vector store)
- Prompt: Formata pergunta + contexto
- LLM: Gera resposta baseada no contexto
- Parser: Extrai texto do LLM output

Conceitos-chave:
- RunnablePassthrough: passa a pergunta original
- RunnableParallel (via dict): executa retriever e passthrough em paralelo
- LCEL pipe operator: composi√ß√£o declarativa
"""

import os
from dotenv import load_dotenv
from langchain_openai import ChatOpenAI, OpenAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough

load_dotenv()

# 1. Preparar documentos
documentos = [
    "Python √© uma linguagem de programa√ß√£o de alto n√≠vel.",
    "LangChain facilita a constru√ß√£o de aplica√ß√µes com LLMs.",
    "RAG combina busca de documentos com gera√ß√£o de LLMs.",
]

# 2. Criar embeddings e vector store
# Embeddings: Convertidos documentos em vetores num√©ricos
embeddings = OpenAIEmbeddings()

# Vector Store: Armazena documentos com seus embeddings para busca por similaridade
vectorstore = FAISS.from_texts(documentos, embeddings)

# Retriever: Interface para buscar documentos relevantes
# search_kwargs={"k": 2} significa: retornar os 2 documentos mais similares
retriever = vectorstore.as_retriever(search_kwargs={"k": 2})

# 3. Criar prompt template
# Este template tem duas vari√°veis:
# - {context}: documentos retornados pelo retriever
# - {question}: pergunta do usu√°rio (passada por RunnablePassthrough)
prompt_template = """
Voc√™ √© um assistente especializado. Use os documentos fornecidos para responder.

Documentos:
{context}

Pergunta: {question}
Resposta:
"""

prompt = ChatPromptTemplate.from_template(prompt_template)

# 4. Configurar modelo e parser
model = ChatOpenAI(model="gpt-4o-mini")
parser = StrOutputParser()  # Extrai apenas o texto da resposta

# 5. Montar a pipeline LCEL
# Sintaxe explicada:
# {"context": retriever, "question": RunnablePassthrough()}
#   ‚Ü≥ Executa retriever e RunnablePassthrough em PARALELO
#   ‚Ü≥ retriever busca docs baseado no input
#   ‚Ü≥ RunnablePassthrough() passa a pergunta original intacta
#   ‚Ü≥ Resultado: {"context": [...docs...], "question": "...pergunta..."}
#
# | prompt ‚Ü≥ Formata as vari√°veis no template
# | model  ‚Ü≥ Envia para o LLM
# | parser ‚Ü≥ Extrai texto da resposta
rag_chain = (
    {
        "context": retriever,              # Busca K docs mais similares
        "question": RunnablePassthrough()  # Mant√©m pergunta original
    }
    | prompt   # ChatPromptTemplate
    | model    # ChatOpenAI
    | parser   # StrOutputParser - retorna apenas texto
)

# 6. Usar a pipeline
pergunta = "O que √© RAG?"
resposta = rag_chain.invoke(pergunta)
print(f"Resposta: {resposta}")

# 7. Streaming (um dos benef√≠cios do LCEL)
# Suportado automaticamente porque todos os componentes s√£o Runnables
print("\nStreaming:")
for chunk in rag_chain.stream(pergunta):
    print(chunk, end="", flush=True)
```

**O que acontece aqui?**

1. **Retriever**: Busca documentos relevantes (dict `{"context": ...}`)
2. **Prompt**: Formata a pergunta e contexto
3. **Model**: Gera resposta
4. **Parser**: Extrai texto da resposta

Tudo isso declarativamente, sem loops ou c√≥digo intermedi√°rio.

### 4.6 Composi√ß√£o Avan√ßada: LLMChain com Memory

LCEL tamb√©m pode incluir l√≥gica de mem√≥ria usando **RunnableLambda**:

```python
from langchain_core.runnables import RunnableLambda

"""
Exemplo: Pipeline com Mem√≥ria Simplificada

Este exemplo mostra como usar RunnableLambda para adicionar l√≥gica customizada
(neste caso, manuten√ß√£o de hist√≥rico) √† cadeia LCEL.

RunnableLambda transforma uma fun√ß√£o Python comum em um Runnable que:
- Integra-se naturalmente √† cadeia
- Suporta streaming automaticamente
- Pode ser testado independentemente
"""

# Mem√≥ria simplificada (em produ√ß√£o, use LangChain's memory classes)
mensagens = []

def adicionar_historico(entrada):
    """
    Fun√ß√£o customizada que adiciona entrada ao hist√≥rico.

    Recebe: string (pergunta do usu√°rio)
    Retorna: dict com hist√≥rico e entrada (para o prompt usar)

    Nota: RunnableLambda envolve esta fun√ß√£o para integr√°-la √† cadeia
    """
    mensagens.append(entrada)
    # Retorna dicion√°rio com as vari√°veis que o prompt precisa
    return {
        "historico": "\n".join(mensagens[-5:]),  # √öltimas 5 mensagens
        "entrada": entrada                        # Pergunta atual
    }

# Pipeline com mem√≥ria
# 1. RunnablePassthrough(): passa a pergunta original
# 2. RunnableLambda(adicionar_historico): fun√ß√£o customizada que mant√©m hist√≥rico
#    - Recebe a pergunta
#    - Adiciona ao hist√≥rico
#    - Retorna {"historico": "...", "entrada": "..."}
# 3. prompt: template usa {historico} e {entrada}
# 4. model: gera resposta
# 5. parser: extrai texto
chat_chain = (
    RunnablePassthrough()                        # Passa pergunta intacta
    | RunnableLambda(adicionar_historico)        # Adiciona l√≥gica customizada (mem√≥ria)
    | prompt                                      # Formata com vari√°veis
    | model                                       # LLM gera resposta
    | parser                                      # Extrai texto
)

# Testando a pipeline com mem√≥ria
resposta1 = chat_chain.invoke("Ol√°, qual √© seu nome?")
print(f"Resposta 1: {resposta1}")

# Agora, adicionar_historico ter√° a pergunta anterior no hist√≥rico
resposta2 = chat_chain.invoke("O que voc√™ acabou de me contar?")
print(f"Resposta 2: {resposta2}")

# Observa√ß√£o: O hist√≥rico agora cont√©m ambas as mensagens
print(f"Hist√≥rico completo: {mensagens}")
```

**Por que RunnableLambda √© poderoso aqui?**
- Transforma uma fun√ß√£o Python em um Runnable
- Integra-se perfeitamente com LCEL
- Suporta automaticamente `.stream()`, `.batch()`, opera√ß√µes ass√≠ncronas
- Sem precisar de classes ou c√≥digo boilerplate

### 4.7 Por Que LCEL N√£o √â Suficiente: Motiva√ß√£o para LangGraph

LCEL √© excelente para **pipelines determin√≠sticas lineares**, mas tem limita√ß√µes importantes quando voc√™ precisa de l√≥gica mais complexa:

| Caracter√≠stica | LCEL | LangGraph |
|---|---|---|
| **Pipes lineares** | ‚úÖ Perfeito | ‚úÖ Perfeito |
| **Streaming** | ‚úÖ Autom√°tico | ‚úÖ Autom√°tico |
| **Loops/Itera√ß√µes** | ‚ùå Imposs√≠vel | ‚úÖ Nativo |
| **Decis√µes condicionais** | ‚ùå Imposs√≠vel | ‚úÖ Nativo |
| **Estado complexo (TypedDict)** | ‚ùå Apenas vari√°veis simples | ‚úÖ Estados tipados |
| **Persist√™ncia/Checkpointing** | ‚ùå N√£o | ‚úÖ Sim |
| **Human-in-the-Loop** | ‚ùå N√£o | ‚úÖ Sim (pausar, retomar) |
| **Multi-agente / paraleliza√ß√£o** | ‚ùå N√£o (apenas serial) | ‚úÖ Sim |

**Exemplo pr√°tico de limita√ß√£o**: Um agente ReAct que faz o ciclo:
1. **Think** (raciocina)
2. **Act** (executa a√ß√£o/tool)
3. **Observe** (observa resultado)
4. **Decide** (continua ou para?)

Este ciclo √© **imposs√≠vel em LCEL puro** porque n√£o h√° suporte para loops. √â aqui que **LangGraph** entra, permitindo estados, n√≥s e arestas para representar fluxos complexos.

> **üìö Refer√™ncia**: O Cap√≠tulo 6 (LangGraph) explora como construir agents e workflows com loops e decis√µes condicionais.

### 4.8 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- O que √© **LCEL** e por que simplifica composi√ß√£o
- Como usar o operador **pipe (`|`)** para encadear componentes
- Construir um **RAG simples com LCEL**
- Limita√ß√µes do LCEL (loops, decis√µes complexas)
- **Pr√≥ximo passo**: LangGraph para agentes com ciclos

### 4.9 Exerc√≠cios

1. **Modifique o RAG**: Adicione um n√≥ de pr√©-processamento que converte a pergunta em 3 varia√ß√µes antes de fazer a busca.

2. **Reuse de chains**: Crie uma fun√ß√£o que retorna chains reutiliz√°veis para diferentes tarefas (Q&A, sumariza√ß√£o, tradu√ß√£o).

3. **Streaming**: Implemente um chatbot simples que usa LCEL com streaming de respostas.

---

## Cap√≠tulo 5: Modularidade e Interoperabilidade

### 5.1 Separa√ß√£o de Pacotes no LangChain v1.0

Um grande problema das vers√µes anteriores era: "Para usar OpenAI, instalo `langchain` e `openai`?". A resposta era confusa.

**LangChain v1.0+** resolve isso com uma arquitetura modular clara:

```
langchain-core
‚îú‚îÄ‚îÄ Tipos, mensagens, LCEL
‚îî‚îÄ‚îÄ Interfaces abstratas (LLM, ChatModel, Tool, etc.)

langchain
‚îú‚îÄ‚îÄ Constru√ß√µes de alto n√≠vel
‚îî‚îÄ‚îÄ Abstra√ß√µes agn√≥sticas

langchain-openai
‚îú‚îÄ‚îÄ ChatOpenAI, OpenAIEmbeddings
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de OpenAI

langchain-anthropic
‚îú‚îÄ‚îÄ ChatAnthropic
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de Anthropic

langchain-google-genai
‚îú‚îÄ‚îÄ ChatGoogleGenerativeAI
‚îî‚îÄ‚îÄ Implementa√ß√£o espec√≠fica de Google

langchain-community
‚îú‚îÄ‚îÄ Conectores mantidos pela comunidade
‚îî‚îÄ‚îÄ Integra√ß√µes experimentais
```

**Benef√≠cio**: Voc√™ instala apenas o que precisa.

```bash
# Uso com OpenAI
pip install langchain langchain-core langchain-openai

# Trocar para Anthropic (sem quebrar seu c√≥digo LCEL)
pip uninstall langchain-openai
pip install langchain-anthropic
# S√≥ troca a importa√ß√£o: ChatOpenAI ‚Üí ChatAnthropic
```

### 5.2 Standard Content Blocks (Interoperabilidade de Output)

Diferentes provedores retornam respostas em formatos diferentes. **Standard Content Blocks** (v1.0) normalizam isso.

Uma mensagem agora pode conter m√∫ltiplos blocos estruturados:

```python
from langchain_core.messages import AIMessage

# Output normalizado (funciona com OpenAI, Claude, Gemini)
message = AIMessage(
    content="Aqui est√° a resposta",
    content_blocks=[
        {"type": "text", "text": "Resposta principal"},
        {"type": "reasoning", "text": "Meu racioc√≠nio..."},
        {"type": "tool_call", "tool": "search", "args": {...}},
        {"type": "citation", "source": "documento_1.pdf"}
    ]
)
```

Isso permite que **uma ferramenta de UI** renderize respostas de qualquer modelo sem mudan√ßas:

```python
def renderizar_resposta(message: AIMessage):
    """Renderiza qualquer mensagem de qualquer provedor igual."""
    for block in message.content_blocks:
        if block["type"] == "text":
            print(block["text"])
        elif block["type"] == "reasoning":
            print(f"[Racioc√≠nio] {block['text']}")
        elif block["type"] == "tool_call":
            print(f"[Tool] {block['tool']}")
```

### 5.3 Portabilidade: Trocar Provedores sem Refatora√ß√£o

Gra√ßas √† separa√ß√£o de pacotes e Standard Content Blocks, seu c√≥digo se torna **agn√≥stico ao provedor**:

```python
# config.py
import os
from langchain_core.language_model import LLM

def get_model() -> LLM:
    """Factory que retorna o modelo configurado."""
    provider = os.getenv("LLM_PROVIDER", "openai")

    if provider == "openai":
        from langchain_openai import ChatOpenAI
        return ChatOpenAI(model="gpt-4o-mini")

    elif provider == "anthropic":
        from langchain_anthropic import ChatAnthropic
        return ChatAnthropic(model="claude-3-5-sonnet")

    elif provider == "google":
        from langchain_google_genai import ChatGoogleGenerativeAI
        return ChatGoogleGenerativeAI(model="gemini-1.5-pro")

    else:
        raise ValueError(f"Provider {provider} n√£o suportado")

# seu_app.py
from config import get_model

model = get_model()
resposta = model.invoke([...])  # Funciona com qualquer modelo!
```

**Uso**:

```bash
# Usar OpenAI
LLM_PROVIDER=openai python seu_app.py

# Trocar para Anthropic (sem mexer no c√≥digo!)
LLM_PROVIDER=anthropic python seu_app.py

# Trocar para Google (sem mexer no c√≥digo!)
LLM_PROVIDER=google python seu_app.py
```

### 5.4 Exemplo: Pipeline Multi-Provider

Construa um pipeline que usa m√∫ltiplos provedores:

```python
# pipeline_multi_provider.py
import os
from dotenv import load_dotenv
from config import get_model
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser

load_dotenv()

# Prompts espec√≠ficos para cada tarefa
analise_prompt = ChatPromptTemplate.from_template(
    "Analise criticamente: {texto}"
)

review_prompt = ChatPromptTemplate.from_template(
    "Revise e aprove ou critique: {analise}"
)

# Modelos: use diferentes provedores para diferentes tarefas
modelo_analista = get_model()  # Padr√£o: OpenAI (r√°pido)
modelo_critico = get_model()   # Poderia ser Anthropic (mais cuidadoso)

# Pipeline
pipeline = (
    {"texto": RunnablePassthrough()}
    | analise_prompt
    | modelo_analista
    | StrOutputParser()
    | {"analise": RunnablePassthrough()}
    | review_prompt
    | modelo_critico
    | StrOutputParser()
)

resultado = pipeline.invoke("Escreva uma proposta de neg√≥cio")
print(f"Resultado final:\n{resultado}")
```

### 5.5 Boas Pr√°ticas: Depend√™ncias M√≠nimas

Ao desenvolver bibliotecas ou aplica√ß√µes, siga este padr√£o:

```python
# Seu pacote: requirements.txt
langchain-core>=1.0.0  # Apenas abstra√ß√µes
pydantic>=2.0

# Seu c√≥digo: suporta m√∫ltiplos provedores
def processar_com_llm(model: Optional[LLM] = None):
    """
    Se model √© None, usa OpenAI. Caso contr√°rio, usa o model passado.
    Assim, o usu√°rio pode injetar qualquer modelo.
    """
    if model is None:
        from langchain_openai import ChatOpenAI
        model = ChatOpenAI()

    # Use model de forma agn√≥stica
    return model.invoke(...)
```

### 5.6 Resumo do Cap√≠tulo

Neste cap√≠tulo, voc√™ aprendeu:

- A **arquitetura modular** do LangChain v1.0+
- Como **separar pacotes** por provedor
- **Standard Content Blocks** para interoperabilidade
- Como construir c√≥digo **agn√≥stico ao provedor**
- Boas pr√°ticas para **depend√™ncias m√≠nimas**

### 5.7 Exerc√≠cios

1. **Factory pattern**: Crie uma classe `LLMFactory` que instancia modelos baseado em vari√°veis de ambiente.

2. **Multi-provider pipeline**: Construa um pipeline que usa OpenAI para rascunho e Anthropic para revis√£o.

3. **Teste de portabilidade**: Implemente um teste que roda o mesmo c√≥digo com 3 provedores diferentes.
